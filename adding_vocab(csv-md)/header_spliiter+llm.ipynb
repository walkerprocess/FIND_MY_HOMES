{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d932245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bc304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-text-splitter (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-text-splitter\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-text-splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733968b",
   "metadata": {},
   "source": [
    "# Markdown Header Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75721100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02e99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}, page_content='Hi this is Jim  \\nHi this is Joe'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}, page_content='Hi this is Lance'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Hi this is Molly')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7edf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='document_result.md')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e257f9c",
   "metadata": {},
   "source": [
    "## Dividing into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "def split_markdown_document(input_file_path, chunk_size=500, chunk_overlap=30):\n",
    "    \"\"\"\n",
    "    Split a markdown document into chunks based on headers and character count.\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "    \n",
    "    # Read the markdown document\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_document = file.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # First split by headers\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    \n",
    "    # Then split by character count\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    # Split the documents\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return splits, markdown_document\n",
    "\n",
    "def display_chunks(splits, original_document):\n",
    "    \"\"\"\n",
    "    Display the original document and the resulting chunks.\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of document chunks\n",
    "        original_document (str): Original markdown document\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DOCUMENT SPLIT INTO {len(splits)} CHUNKS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(splits):\n",
    "        print(f\"\\nCHUNK {i+1} (Length: {len(chunk.page_content)} characters):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Print metadata\n",
    "        print(\"Metadata:\")\n",
    "        for key, value in chunk.metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Print content\n",
    "        print(\"\\nContent:\")\n",
    "        print(chunk.page_content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    # Define input file path\n",
    "    input_file = \"합쳐진_용어집.md\"\n",
    "    \n",
    "    # Define chunk size\n",
    "    chunk_size = int(1000)\n",
    "    \n",
    "    # Define chunk overlap\n",
    "    chunk_overlap = int(40)\n",
    "    \n",
    "    # Split the document\n",
    "    try:\n",
    "        splits, original_document = split_markdown_document(input_file, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Display the results\n",
    "        display_chunks(splits, original_document)\n",
    "        \n",
    "        print(f\"\\nTotal chunks created: {len(splits)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb3237",
   "metadata": {},
   "source": [
    "# Header Splitter + LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "def split_markdown_document(input_file_path, chunk_size=500, chunk_overlap=30):\n",
    "    \"\"\"\n",
    "    Split a markdown document into chunks based on headers and character count.\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks and original document\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "    \n",
    "    # Read the markdown document\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_document = file.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # First split by headers\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    \n",
    "    # Then split by character count\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    # Split the documents\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return splits, markdown_document\n",
    "\n",
    "def process_file(chunk_content, chunk_number):\n",
    "    \"\"\"\n",
    "    Process a chunk of content through Azure AI to convert HTML tables to text.\n",
    "    \n",
    "    Args:\n",
    "        chunk_content (str): Content of the chunk to process\n",
    "        chunk_number (int): Number of the chunk for identification\n",
    "        \n",
    "    Returns:\n",
    "        str: Processed text content\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    subscription_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    \n",
    "    if not all([endpoint, deployment, subscription_key]):\n",
    "        raise ValueError(\"환경변수 누락: ENDPOINT_URL, DEPLOYMENT_NAME, AZURE_OPENAI_KEY를 확인하세요.\")\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=subscription_key,\n",
    "        api_version=\"2024-05-01-preview\",\n",
    "    )\n",
    "    \n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"너는 HTML 테이블을 읽고 자연스러운 서술형 텍스트로 변환하는 텍스트 변환 엔진이다.\\n\\n입력 데이터는 일반 텍스트와 HTML 코드가 혼합된 문서이며, 이 중 HTML 테이블(`<table>`) 형식으로 작성된 표만을 감지하여 사람이 읽기 쉬운 **자연스러운 텍스트**로 변환하라. 표 외의 일반 텍스트는 **절대로 변경하지 않는다**. \\n\\n출력된 텍스트는 아래의 기준을 모두 따라야 한다:\\n\\n1. 표의 계층 구조, 제목, 셀의 관계를 모두 파악하여 자연어로 기술한다.\\n2. 셀이 병합된 경우 (`rowspan`, `colspan`)에는 의미적으로 내용을 통합하여 풀어서 설명한다.\\n3. 표 안에 또 다른 표가 중첩되어 있는 경우에도 각 표를 계층적으로 처리하고, 문맥상 자연스럽게 연결되도록 한다.\\n4. 빈 칸이 있는 경우, 내용을 유추하지 않고 \\\"(빈칸)\\\" 또는 \\\"해당 없음\\\" 등으로 명확하게 표기한다.\\n5. 항목 간 구분은 \\\"■\\\", \\\"1.\\\", \\\"-\\\" 등을 사용하여 명확히 구분하고, 계층적으로 정리한다.\\n6. 결과 텍스트는 반드시 문맥상 자연스럽고 일관되게 연결되어야 하며, 원래 문서의 흐름과 연결되도록 이어져야 한다.\\n7. HTML 태그가 아닌 일반 텍스트 영역은 절대로 수정하거나 재구성하지 않는다.\\n8. 결과는 마크다운 문서로 사용 가능한 수준의 가독성을 갖춰야 하며, 표를 설명하는 문장은 공식 문서나 계약서 스타일처럼 명료하고 단정하게 작성한다.\\n\\n예외나 애매한 구조가 있어도 최대한 의미를 보존하여 사람이 이해할 수 있도록 직관적으로 설명하라.\\n\\n입력 형식 예시:\\n(본문 텍스트)\\n<table>...</table>\\n(본문 텍스트 계속)\\n\\n출력 형식 예시:\\n(본문 텍스트)\\n■ 항목명  \\n- 내용1  \\n- 내용2  \\n이제 아래에 입력된 문서 내 HTML 테이블을 위 기준에 따라 서술형 텍스트로 변환하라.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": chunk_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=chat_prompt,\n",
    "        max_tokens=1500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    processed_text = completion.choices[0].message.content\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def prepare_for_vector_db(processed_text, chunk_metadata):\n",
    "    \"\"\"\n",
    "    Prepare processed text for vector DB storage.\n",
    "    \n",
    "    Args:\n",
    "        processed_text (str): The processed text\n",
    "        chunk_metadata (dict): The metadata for this chunk\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary formatted for vector DB insertion\n",
    "    \"\"\"\n",
    "    # Create a dict that can be easily stored in vector DB\n",
    "    vector_db_entry = {\n",
    "        \"text\": processed_text,\n",
    "        \"metadata\": chunk_metadata\n",
    "    }\n",
    "    \n",
    "    return vector_db_entry\n",
    "\n",
    "def process_document(input_file_path, output_dir=\"processed_chunks\"):\n",
    "    \"\"\"\n",
    "    Main function to process a document:\n",
    "    1. Split document into chunks\n",
    "    2. Process each chunk with Azure OpenAI\n",
    "    3. Prepare for vector DB\n",
    "    4. Save processed chunks to text files\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        output_dir (str): Directory to save processed chunks\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Step 1: Split the document into chunks\n",
    "    print(f\"🔄 Splitting document '{input_file_path}' into chunks...\")\n",
    "    chunks, original_document = split_markdown_document(input_file_path)\n",
    "    print(f\"✅ Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # List to store vector DB entries\n",
    "    vector_db_entries = []\n",
    "    \n",
    "    # Step 2 & 3: Process each chunk and prepare for vector DB\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_number = i + 1\n",
    "        print(f\"\\n🔄 Processing chunk {chunk_number} of {len(chunks)}...\")\n",
    "        \n",
    "        # Save original chunk to file\n",
    "        original_chunk_path = os.path.join(output_dir, f\"chunk_{chunk_number}_original.txt\")\n",
    "        with open(original_chunk_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk.page_content)\n",
    "        \n",
    "        # Process the chunk with Azure OpenAI\n",
    "        processed_text = process_file(chunk.page_content, chunk_number)\n",
    "        \n",
    "        # Prepare for vector DB\n",
    "        vector_db_entry = prepare_for_vector_db(processed_text, chunk.metadata)\n",
    "        vector_db_entries.append(vector_db_entry)\n",
    "        \n",
    "        # Step 4: Save processed chunk to text file\n",
    "        processed_chunk_path = os.path.join(output_dir, f\"chunk_{chunk_number}_processed.txt\")\n",
    "        with open(processed_chunk_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(processed_text)\n",
    "        \n",
    "        # Save vector DB entry to JSON file\n",
    "        vector_db_entry_path = os.path.join(output_dir, f\"chunk_{chunk_number}_vector_db.json\")\n",
    "        with open(vector_db_entry_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vector_db_entry, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✅ Chunk {chunk_number} processed and saved\")\n",
    "    \n",
    "    # Save all vector DB entries to a single file\n",
    "    all_entries_path = os.path.join(output_dir, \"all_vector_db_entries.json\")\n",
    "    with open(all_entries_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vector_db_entries, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ All processing complete! Results saved in '{output_dir}'\")\n",
    "    print(f\"✅ Vector DB entries saved to '{all_entries_path}'\")\n",
    "    \n",
    "    return vector_db_entries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your input file path here\n",
    "    input_file_path = \"document_result.md\"  # Change this to your input file path\n",
    "    \n",
    "    # Process the document\n",
    "    vector_db_entries = process_document(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d6cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
