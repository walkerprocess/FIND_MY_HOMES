{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d932245",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bc304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-text-splitter (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-text-splitter\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-text-splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733968b",
   "metadata": {},
   "source": [
    "# Markdown Header Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75721100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02e99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}, page_content='Hi this is Jim  \\nHi this is Joe'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}, page_content='Hi this is Lance'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Hi this is Molly')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7edf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='document_result.md')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e257f9c",
   "metadata": {},
   "source": [
    "## Dividing into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "def split_markdown_document(input_file_path, chunk_size=500, chunk_overlap=30):\n",
    "    \"\"\"\n",
    "    Split a markdown document into chunks based on headers and character count.\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "    \n",
    "    # Read the markdown document\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_document = file.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # First split by headers\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    \n",
    "    # Then split by character count\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    # Split the documents\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return splits, markdown_document\n",
    "\n",
    "def display_chunks(splits, original_document):\n",
    "    \"\"\"\n",
    "    Display the original document and the resulting chunks.\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of document chunks\n",
    "        original_document (str): Original markdown document\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DOCUMENT SPLIT INTO {len(splits)} CHUNKS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(splits):\n",
    "        print(f\"\\nCHUNK {i+1} (Length: {len(chunk.page_content)} characters):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Print metadata\n",
    "        print(\"Metadata:\")\n",
    "        for key, value in chunk.metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Print content\n",
    "        print(\"\\nContent:\")\n",
    "        print(chunk.page_content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    # Define input file path\n",
    "    input_file = \"í•©ì³ì§„_ìš©ì–´ì§‘.md\"\n",
    "    \n",
    "    # Define chunk size\n",
    "    chunk_size = int(1000)\n",
    "    \n",
    "    # Define chunk overlap\n",
    "    chunk_overlap = int(40)\n",
    "    \n",
    "    # Split the document\n",
    "    try:\n",
    "        splits, original_document = split_markdown_document(input_file, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Display the results\n",
    "        display_chunks(splits, original_document)\n",
    "        \n",
    "        print(f\"\\nTotal chunks created: {len(splits)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb3237",
   "metadata": {},
   "source": [
    "# Header Splitter + LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "def split_markdown_document(input_file_path, chunk_size=500, chunk_overlap=30):\n",
    "    \"\"\"\n",
    "    Split a markdown document into chunks based on headers and character count.\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks and original document\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(input_file_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "    \n",
    "    # Read the markdown document\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        markdown_document = file.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # First split by headers\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "    \n",
    "    # Then split by character count\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        keep_separator=True\n",
    "    )\n",
    "    \n",
    "    # Split the documents\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "    return splits, markdown_document\n",
    "\n",
    "def process_file(chunk_content, chunk_number):\n",
    "    \"\"\"\n",
    "    Process a chunk of content through Azure AI to convert HTML tables to text.\n",
    "    \n",
    "    Args:\n",
    "        chunk_content (str): Content of the chunk to process\n",
    "        chunk_number (int): Number of the chunk for identification\n",
    "        \n",
    "    Returns:\n",
    "        str: Processed text content\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    subscription_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    \n",
    "    if not all([endpoint, deployment, subscription_key]):\n",
    "        raise ValueError(\"í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: ENDPOINT_URL, DEPLOYMENT_NAME, AZURE_OPENAI_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=subscription_key,\n",
    "        api_version=\"2024-05-01-preview\",\n",
    "    )\n",
    "    \n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"ë„ˆëŠ” HTML í…Œì´ë¸”ì„ ì½ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ í˜• í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í…ìŠ¤íŠ¸ ë³€í™˜ ì—”ì§„ì´ë‹¤.\\n\\nì…ë ¥ ë°ì´í„°ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ì™€ HTML ì½”ë“œê°€ í˜¼í•©ëœ ë¬¸ì„œì´ë©°, ì´ ì¤‘ HTML í…Œì´ë¸”(`<table>`) í˜•ì‹ìœ¼ë¡œ ì‘ì„±ëœ í‘œë§Œì„ ê°ì§€í•˜ì—¬ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ **ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸**ë¡œ ë³€í™˜í•˜ë¼. í‘œ ì™¸ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” **ì ˆëŒ€ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤**. \\n\\nì¶œë ¥ëœ í…ìŠ¤íŠ¸ëŠ” ì•„ë˜ì˜ ê¸°ì¤€ì„ ëª¨ë‘ ë”°ë¼ì•¼ í•œë‹¤:\\n\\n1. í‘œì˜ ê³„ì¸µ êµ¬ì¡°, ì œëª©, ì…€ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ íŒŒì•…í•˜ì—¬ ìì—°ì–´ë¡œ ê¸°ìˆ í•œë‹¤.\\n2. ì…€ì´ ë³‘í•©ëœ ê²½ìš° (`rowspan`, `colspan`)ì—ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë‚´ìš©ì„ í†µí•©í•˜ì—¬ í’€ì–´ì„œ ì„¤ëª…í•œë‹¤.\\n3. í‘œ ì•ˆì— ë˜ ë‹¤ë¥¸ í‘œê°€ ì¤‘ì²©ë˜ì–´ ìˆëŠ” ê²½ìš°ì—ë„ ê° í‘œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•œë‹¤.\\n4. ë¹ˆ ì¹¸ì´ ìˆëŠ” ê²½ìš°, ë‚´ìš©ì„ ìœ ì¶”í•˜ì§€ ì•Šê³  \\\"(ë¹ˆì¹¸)\\\" ë˜ëŠ” \\\"í•´ë‹¹ ì—†ìŒ\\\" ë“±ìœ¼ë¡œ ëª…í™•í•˜ê²Œ í‘œê¸°í•œë‹¤.\\n5. í•­ëª© ê°„ êµ¬ë¶„ì€ \\\"â– \\\", \\\"1.\\\", \\\"-\\\" ë“±ì„ ì‚¬ìš©í•˜ì—¬ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , ê³„ì¸µì ìœ¼ë¡œ ì •ë¦¬í•œë‹¤.\\n6. ê²°ê³¼ í…ìŠ¤íŠ¸ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ë˜ê²Œ ì—°ê²°ë˜ì–´ì•¼ í•˜ë©°, ì›ë˜ ë¬¸ì„œì˜ íë¦„ê³¼ ì—°ê²°ë˜ë„ë¡ ì´ì–´ì ¸ì•¼ í•œë‹¤.\\n7. HTML íƒœê·¸ê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì˜ì—­ì€ ì ˆëŒ€ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¬êµ¬ì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.\\n8. ê²°ê³¼ëŠ” ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ê°€ë…ì„±ì„ ê°–ì¶°ì•¼ í•˜ë©°, í‘œë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì€ ê³µì‹ ë¬¸ì„œë‚˜ ê³„ì•½ì„œ ìŠ¤íƒ€ì¼ì²˜ëŸ¼ ëª…ë£Œí•˜ê³  ë‹¨ì •í•˜ê²Œ ì‘ì„±í•œë‹¤.\\n\\nì˜ˆì™¸ë‚˜ ì• ë§¤í•œ êµ¬ì¡°ê°€ ìˆì–´ë„ ìµœëŒ€í•œ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ì—¬ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ë¼.\\n\\nì…ë ¥ í˜•ì‹ ì˜ˆì‹œ:\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸)\\n<table>...</table>\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê³„ì†)\\n\\nì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸)\\nâ–  í•­ëª©ëª…  \\n- ë‚´ìš©1  \\n- ë‚´ìš©2  \\nì´ì œ ì•„ë˜ì— ì…ë ¥ëœ ë¬¸ì„œ ë‚´ HTML í…Œì´ë¸”ì„ ìœ„ ê¸°ì¤€ì— ë”°ë¼ ì„œìˆ í˜• í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ë¼.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": chunk_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=chat_prompt,\n",
    "        max_tokens=1500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    processed_text = completion.choices[0].message.content\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def prepare_for_vector_db(processed_text, chunk_metadata):\n",
    "    \"\"\"\n",
    "    Prepare processed text for vector DB storage.\n",
    "    \n",
    "    Args:\n",
    "        processed_text (str): The processed text\n",
    "        chunk_metadata (dict): The metadata for this chunk\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary formatted for vector DB insertion\n",
    "    \"\"\"\n",
    "    # Create a dict that can be easily stored in vector DB\n",
    "    vector_db_entry = {\n",
    "        \"text\": processed_text,\n",
    "        \"metadata\": chunk_metadata\n",
    "    }\n",
    "    \n",
    "    return vector_db_entry\n",
    "\n",
    "def process_document(input_file_path, output_dir=\"processed_chunks\"):\n",
    "    \"\"\"\n",
    "    Main function to process a document:\n",
    "    1. Split document into chunks\n",
    "    2. Process each chunk with Azure OpenAI\n",
    "    3. Prepare for vector DB\n",
    "    4. Save processed chunks to text files\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input markdown file\n",
    "        output_dir (str): Directory to save processed chunks\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Step 1: Split the document into chunks\n",
    "    print(f\"ğŸ”„ Splitting document '{input_file_path}' into chunks...\")\n",
    "    chunks, original_document = split_markdown_document(input_file_path)\n",
    "    print(f\"âœ… Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # List to store vector DB entries\n",
    "    vector_db_entries = []\n",
    "    \n",
    "    # Step 2 & 3: Process each chunk and prepare for vector DB\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_number = i + 1\n",
    "        print(f\"\\nğŸ”„ Processing chunk {chunk_number} of {len(chunks)}...\")\n",
    "        \n",
    "        # Save original chunk to file\n",
    "        original_chunk_path = os.path.join(output_dir, f\"chunk_{chunk_number}_original.txt\")\n",
    "        with open(original_chunk_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk.page_content)\n",
    "        \n",
    "        # Process the chunk with Azure OpenAI\n",
    "        processed_text = process_file(chunk.page_content, chunk_number)\n",
    "        \n",
    "        # Prepare for vector DB\n",
    "        vector_db_entry = prepare_for_vector_db(processed_text, chunk.metadata)\n",
    "        vector_db_entries.append(vector_db_entry)\n",
    "        \n",
    "        # Step 4: Save processed chunk to text file\n",
    "        processed_chunk_path = os.path.join(output_dir, f\"chunk_{chunk_number}_processed.txt\")\n",
    "        with open(processed_chunk_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(processed_text)\n",
    "        \n",
    "        # Save vector DB entry to JSON file\n",
    "        vector_db_entry_path = os.path.join(output_dir, f\"chunk_{chunk_number}_vector_db.json\")\n",
    "        with open(vector_db_entry_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vector_db_entry, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Chunk {chunk_number} processed and saved\")\n",
    "    \n",
    "    # Save all vector DB entries to a single file\n",
    "    all_entries_path = os.path.join(output_dir, \"all_vector_db_entries.json\")\n",
    "    with open(all_entries_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vector_db_entries, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… All processing complete! Results saved in '{output_dir}'\")\n",
    "    print(f\"âœ… Vector DB entries saved to '{all_entries_path}'\")\n",
    "    \n",
    "    return vector_db_entries\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your input file path here\n",
    "    input_file_path = \"document_result.md\"  # Change this to your input file path\n",
    "    \n",
    "    # Process the document\n",
    "    vector_db_entries = process_document(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d6cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
