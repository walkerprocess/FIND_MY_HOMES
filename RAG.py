from dotenv import load_dotenv
from QR import query_rewrite
import os
import requests
import re
import json
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch
from public_notice import doc_links

# .env ë¡œë”©
load_dotenv()

# í™˜ê²½ë³€ìˆ˜
embedding_api_key = os.getenv('Embedding_API_KEY')
embedding_endpoint = os.getenv('Embedding_ENDPOINT')
embedding_api_version = os.getenv('embedding_api_version')
embedding_deployment = os.getenv('embedding_deployment')
ai_search_endpoint = os.getenv("pdf_vocab_gh_fixed_new_index_Search_ENDPOINT")
ai_search_api_key = os.getenv('AI_Search_API_KEY')
#llm_endpoint = os.getenv('OPENAI_ENDPOINT')
#llm_api_key = os.getenv('OPENAI_API_KEY')
llm_endpoint = os.getenv('OPENAI_ENDPOINT_2')
llm_api_key = os.getenv('OPENAI_API_KEY_2')

# ì„ë² ë”© ê°ì²´
embedding = AzureOpenAIEmbeddings(
    api_key = embedding_api_key,
    azure_endpoint = embedding_endpoint,
    model = embedding_deployment,
    openai_api_version = embedding_api_version
)

# ë²¡í„° ê²€ìƒ‰
def request_ai_search(query: str, source_filter: str = None, k: int = 5) -> list:
    headers = {
        "Content-Type": "application/json",
        "api-key": ai_search_api_key
    }

    query_vector = embedding.embed_query(query)

    body = {
        "search": query,
        "vectorQueries": [
            {
                "kind": "vector",
                "vector": query_vector,
                "fields": "embedding",
                "k": k
            }
        ]
    }

    if source_filter:
        cleaned_source = source_filter.replace(".pdf", "")
        body["filter"] = f"source eq '{cleaned_source}'"

    response = requests.post(ai_search_endpoint, headers=headers, json=body)

    if response.status_code != 200:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
        print(response.text)
        return []

    return [
        {
            "content": item["content"],
            "source": item.get("source", ""),
            "score": item.get("@search.score", 0)
        }
        for item in response.json()["value"]
    ]

# GPT ì‘ë‹µ ìš”ì²­
def request_gpt(prompt: str) -> str:
    headers = {
        'Content-Type': 'application/json',
        'api-key': llm_api_key
    }

    body = {
        "messages": [
            {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë„ìš°ë¯¸ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ì— ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•´ì¤˜."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800
    }

    response = requests.post(llm_endpoint, headers=headers, json=body)
    if response.status_code == 200:
        content = response.json()['choices'][0]['message']['content']
        return re.sub(r'\[doc(\d+)\]', r'[ì°¸ì¡° \1]', content)
    else:
        print("âŒ GPT ìš”ì²­ ì‹¤íŒ¨:", response.status_code, response.text)
        return "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ë§ˆí¬ë‹¤ìš´ ì œê±° í›„ì²˜ë¦¬
def remove_markdown(text: str) -> str:
    # 1) í—¤ë”(# ...) ì œê±°
    text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
    
    # 2) ë³¼ë“œ/ì´íƒ¤ë¦­(*...* ë˜ëŠ” **...**) ì œê±°
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    
    # 3) ì–¸ë”ìŠ¤ì½”ì–´(_..._) ì œê±°
    text = re.sub(r'_{1,2}([^_]+)_{1,2}', r'\1', text)
    
    # 4) ë‚˜ë¨¸ì§€ ë°±í‹±(`) ë“± ì œê±°
    text = text.replace('`', '')
    
    return text

# ìµœì¢… RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_answer_with_rag(query: str, source_filter: str = None, top_k: int = 3) -> str:
    results = request_ai_search(query, source_filter=source_filter, k=top_k)
    if not results:
        return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    #context = "\n\n".join([f"[doc{i+1}]\n{item['content']}" for i, item in enumerate(results)])
    context = "\n\n".join([f"[{item['source']}]\n{item['content']}" for item in results])
    prompt = f"""You are an expert AI assistant. Based on the document excerpts below, answer the question in clear, concise Korean by organizing your answer into sections. Return the answer in valid JSON format using the following schema:
    
    {{
      "sections": [
        {{
          "title": "Section Title",
          "content": "Section Content"
        }},
        ...
      ]
    }}
    
    Document Excerpts:
    {context}
    
    Question:
    {query}
    
    Answer in Korean as valid JSON without using any Markdown syntax:"""
    raw_answer = request_gpt(prompt)
    # ë§Œì•½ LLMì˜ ê²°ê³¼ì— ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì´ ë‚¨ì•„ìˆë‹¤ë©´ í›„ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.
    clean_answer = remove_markdown(raw_answer)

    try:
        answer_json = json.loads(clean_answer)
        sections = answer_json.get("sections", [])
    except json.JSONDecodeError:
        sections = [{"title": "ë‹µë³€", "content": clean_answer}]

    # ê³µê³ ë¬¸ ì •ë³´ ì¹´ë“œ ì‚½ì…
    if source_filter:
        doc_title = source_filter.replace(".pdf", "")
        matched_url = doc_links.get(doc_title, "#")
        sections.insert(0, {
            "title": "ğŸ“„ ì„ íƒí•œ ê³µê³ ë¬¸",
            "content": f"{doc_title}\nğŸ“ ë§í¬: {matched_url}"
        })

    final_answer = json.dumps({"sections": sections}, ensure_ascii=False)
    return final_answer

# RAG ì—†ì´ ìˆœìˆ˜ LLMë§Œ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±
def generate_answer_with_llm(query: str) -> str:
    prompt = f"""You are a professional AI assistant specializing in public policy and housing information in South Korea.

Your task is to answer the user's question clearly, **in Korean**, using accurate and up-to-date knowledge.

Instructions:
- Provide a concise answer within 300 characters.
- Use polite, reader-friendly Korean appropriate for civil service guidance.
- Do **not** use Markdown, special characters (like `#`, `*`, `_`), or emojis.
- Avoid repeating the user's question in your answer.

User's Question:
{query}

Answer (in Korean, under 300 characters):"""
    return request_gpt(prompt)

prompt = '''ê²½ê¸°ë„ ê±°ì£¼,ë‚˜ì´ëŠ” 26ì„¸, ëŒ€í•™ ì¡¸ì—…, ë¬´ì§
            ì œì¶œí•´ì•¼í•  ì„œë¥˜'''
new_prompt = query_rewrite(prompt)
print('ğŸ¶new_prompt',new_prompt)
chunk_result = request_ai_search(new_prompt,source_filter=None)
result = generate_answer_with_rag(new_prompt,source_filter=None)

i = 1
for chunk in chunk_result:
    print('============================')
    print(f'ğŸ¤– top {i} result : {chunk}')
    i += 1
    if i == 10:
        break
    
print('============================')
print('============================')
print(f'ğŸ¤–chunk_resultğŸ¤– = {result}')
print('hi')