{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1830bdab",
   "metadata": {},
   "source": [
    "## PyMuPDF4LLM .md ì „ì²˜ë¦¬\n",
    "- ê³µë°± í…Œì´ë¸” ë³‘í•©\n",
    "- ë³‘í•©ì…€ ì±„ì›Œë„£ê¸°\n",
    "- í˜„ì¬: í•´ë‹¹ í…Œì´ë¸”ì´ ìœ„ì¹˜í•˜ëŠ” í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì—¬ ë³‘í•©ëœ í‘œ(df)ë¥¼ ì–»ìŒ\n",
    "- ì£¼ì˜: ì»¬ëŸ¼ì„ ì˜ëª» ì½ì–´ì˜¨ ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬, í•œ í˜ì´ì§€ì— ì—¬ëŸ¬ í‘œê°€ ìˆëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d767571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "# PDF data ì¶”ì¶œ\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "llama_docs = llama_reader.load_data(r\"data\\pdf_data\\LH-25ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸(ì„œìš¸ì§€ì—­ë³¸ë¶€).pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ce9cb",
   "metadata": {},
   "source": [
    "### 1) ì»¬ëŸ¼ ì˜ˆì™¸ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4172b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "import re\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "special_chars = ['â€¢', 'â– ', 'â€»']\n",
    "\n",
    "def fix_invalid_column_lines(md_text: str) -> str:\n",
    "    lines = md_text.splitlines()\n",
    "    new_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "\n",
    "        # ì¡°ê±´: | 2ê°œ ì´ìƒ + íŠ¹ìˆ˜ê¸°í˜¸ í¬í•¨ + ë‹¤ìŒì¤„ êµ¬ë¶„ì„  + ë‹¤ë‹¤ìŒ ì¤„ ì¡´ì¬\n",
    "        if (\n",
    "            line.count('|') >= 2 and \n",
    "            any(char in line for char in special_chars) and \n",
    "            i + 2 < len(lines)\n",
    "        ):\n",
    "            next_line = lines[i + 1].strip()\n",
    "            next_next_line = lines[i + 2].strip()\n",
    "\n",
    "            if re.fullmatch(r'\\|?[-| ]+\\|?', next_line):\n",
    "                print(\"ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½:\", line)\n",
    "\n",
    "                # ì˜ëª»ëœ ì¤„ (| ì œê±°) ë¨¼ì € ì˜¬ë¦¼\n",
    "                new_lines.append(line.replace('|', '').strip())\n",
    "                # ì‹¤ì œ ì»¬ëŸ¼ëª…\n",
    "                new_lines.append(next_next_line)\n",
    "                # êµ¬ë¶„ì„ \n",
    "                new_lines.append(next_line)\n",
    "\n",
    "                i += 3\n",
    "                continue\n",
    "\n",
    "        # ê·¸ ì™¸ëŠ” ê·¸ëŒ€ë¡œ\n",
    "        new_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    return '\\n'.join(new_lines)\n",
    "\n",
    "# llama_docs ê° ë¬¸ì„œì— ì ìš©\n",
    "cleaned_docs = []\n",
    "for doc in llama_docs:\n",
    "    modified = fix_invalid_column_lines(doc.text)\n",
    "    cleaned_docs.append(Document(text=modified))\n",
    "cleaned_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da15ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'(\\|.*?\\|\\n(\\|[-:]+[-|:]*\\|\\n)(\\|.*?\\|\\n)+)', cleaned_docs[13].text, re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0dc7b",
   "metadata": {},
   "source": [
    "### 2) ê³µë°± í…Œì´ë¸” ì „ì²˜ë¦¬ (í…Œì´ë¸” ë³‘í•© ë° ë³‘í•©ì…€ ì±„ìš°ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def merge_pagetext(docs, page_list):\n",
    "    merged_text = \"\"\n",
    "    for page in page_list:\n",
    "        text = docs[page-1].text\n",
    "        merged_text += text\n",
    "    merged_text = merged_text.replace('-----', '') #  í˜ì´ì§€ êµ¬ë¶„ ì„ \n",
    "\n",
    "    # í˜ì´ì§€ ë„˜ë²„ ë¬¸ìì—´ ì œê±°\n",
    "    for page_num in page_list:\n",
    "        merged_text = merged_text.replace(f\"- {page_num}\", '')\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "def is_table_separator(line):\n",
    "    return re.match(r'^\\s*\\|?[:\\-]+(?:\\|[:\\-]+)*\\|?\\s*$', line.strip())\n",
    "\n",
    "def is_table_row(line):\n",
    "    return re.match(r'^\\s*\\|.*\\|\\s*$', line.strip())\n",
    "\n",
    "def is_ignore_line(line):\n",
    "    # í˜ì´ì§€ ë‚˜ëˆ” ê°™ì€ êµ¬ë¶„ì„ : -----, =====, *** ë“±\n",
    "    return re.match(r'^\\s*[-=*]{3,}\\s*$', line.strip())\n",
    "\n",
    "def extract_combined_tables(text):\n",
    "    lines = text.splitlines()\n",
    "    tables = []\n",
    "    current_table = []\n",
    "    inside_table = False\n",
    "    is_column = False\n",
    "\n",
    "    for line in lines:\n",
    "        if is_table_row(line) or is_table_separator(line):\n",
    "            current_table.append(line.strip())\n",
    "            inside_table = True\n",
    "        elif is_ignore_line(line):\n",
    "            # í˜ì´ì§€ êµ¬ë¶„ ë“±ì€ ë¬´ì‹œí•˜ê³  í‘œ ê³„ì† ì´ì–´ë¶™ì„\n",
    "            continue\n",
    "        elif line.strip() == '':\n",
    "            # ë¹ˆ ì¤„ì€ ë¬´ì‹œ (í‘œ ëŠì§€ ì•ŠìŒ)\n",
    "            continue\n",
    "        else:\n",
    "            # ì¼ë°˜ í…ìŠ¤íŠ¸: í‘œ ëìœ¼ë¡œ ê°„ì£¼\n",
    "            if inside_table and current_table:\n",
    "                tables.append('\\n'.join(current_table))\n",
    "                current_table = []\n",
    "                inside_table = False\n",
    "\n",
    "    # ëì— í‘œê°€ ë‚¨ì•„ ìˆë‹¤ë©´ ì¶”ê°€\n",
    "    if current_table:\n",
    "        tables.append('\\n'.join(current_table))\n",
    "\n",
    "    return tables\n",
    "\n",
    "# í–‰ ìˆ˜ ì„¸ëŠ” í•¨ìˆ˜ (í—¤ë” ì œì™¸, êµ¬ë¶„ì„  ì œì™¸)\n",
    "def count_rows(md_table):\n",
    "    lines = md_table.strip().splitlines()\n",
    "    return max(0, len(lines) - 2)\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì¬êµ¬ì„± í•¨ìˆ˜\n",
    "def make_merged_table_md(merged_text):\n",
    "\n",
    "    # ê° ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "    lines = merged_text.strip().split('\\n')\n",
    "\n",
    "    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ë§Œ í—¤ë” ê³µë°±ê°’ ì „ì²˜ë¦¬\n",
    "    lines[0] = lines[0].replace('||', '|Col|')\n",
    "\n",
    "    # ì—¬ê¸°ì„œ ì§€ìš°ì ì˜ëª» ë“¤ì–´ê°„ êµ¬ë¶„ì„ \n",
    "    \n",
    "\n",
    "    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    merged_table = []\n",
    "    merged_table.extend(lines[:2])\n",
    "    header_found = True\n",
    "    for i in range(2, len(lines)):\n",
    "        line = lines[i]\n",
    "        # print(line)\n",
    "        if '|---|' in line:\n",
    "            merged_table.pop()\n",
    "            line = ''\n",
    "\n",
    "        if line == '':\n",
    "            continue\n",
    "        if re.match(r'^\\|.*\\|$', line.strip()):\n",
    "            # í—¤ë”ê°€ ì•„ì§ ë°œê²¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²« í—¤ë”ì™€ êµ¬ë¶„ì„ ë§Œ ì¶”ê°€\n",
    "            if not header_found and re.match(r'^\\|[^|]+\\|[^|]+\\|*', line):\n",
    "                merged_table.append(line)\n",
    "                header_found = True\n",
    "            # êµ¬ë¶„ì„ ì€ ë¬´ì‹œ (ë‘ ë²ˆì§¸ ì´í›„ëŠ”)\n",
    "            elif '---' in line:\n",
    "                continue\n",
    "            else:\n",
    "                # ë‚˜ë¨¸ì§€ëŠ” ë°ì´í„° í–‰ìœ¼ë¡œ ì²˜ë¦¬\n",
    "                merged_table.append(line)\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥ (ë§ˆí¬ë‹¤ìš´ í‘œ í˜•íƒœ)\n",
    "    return '\\n'.join(merged_table)\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì¬êµ¬ì„± í•¨ìˆ˜\n",
    "def make_merged_table_df(merged_text):\n",
    "    # table_blocks = re.findall(r'(\\|.*?\\|\\n(\\|[-:]+[-|:]*\\|\\n)(\\|.*?\\|\\n)+)', merged_text, re.DOTALL)\n",
    "    # first_block = table_blocks[0]\n",
    "    # table_md = first_block[0]\n",
    "    # rows = table_md.strip().split(\"\\n\")\n",
    "\n",
    "    rows = merged_text.strip().split(\"\\n\")\n",
    "    header = rows[0]  # ì»¬ëŸ¼ëª… (ì²« ë²ˆì§¸ í–‰)\n",
    "    column_list = [col.strip() for col in header.split(\"|\") if col.strip()]\n",
    "    data_rows = rows[2:]\n",
    "\n",
    "    df = pd.read_csv(StringIO(\"\\n\".join([header] + data_rows)), sep=\"|\", engine=\"python\", skipinitialspace=True)\n",
    "    df = df.iloc[:, 1:-1]  # ì•ë’¤ ê³µë°± ì»¬ëŸ¼ ì œê±°\n",
    "    df.columns = column_list \n",
    "    df = df[0:].reset_index(drop=True)  # ë°ì´í„°ë§Œ ìœ ì§€\n",
    "\n",
    "    # ffillì€ ë”°ë¡œ í•´ì£¼ê¸° (ë³‘í•©ì…€ ì‹¬í™” ê³¼ì •- í•¨ìˆ˜ ì•ˆì—ì„œ í•˜ê±°ë‚˜ ë°˜í™˜í•œ í…Œì´ë¸” ì „ì²˜ë¦¬ ë”°ë¡œ)\n",
    "    return df\n",
    "\n",
    "table_df_list = []\n",
    "# page_list = [9, 10, 11]\n",
    "for page_list in extended_page_list:\n",
    "    full_text = merge_pagetext(cleaned_docs, page_list)\n",
    "\n",
    "    table_list = extract_combined_tables(full_text)\n",
    "\n",
    "    # í–‰ì´ ê°€ì¥ ë§ì€ í‘œ í•˜ë‚˜ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "    max_table = max(table_list, key=count_rows)\n",
    "\n",
    "    # í‘œ ì¬êµ¬ì„±\n",
    "    merged_table_md = make_merged_table_md(max_table)\n",
    "    df = make_merged_table_df(merged_table_md)\n",
    "\n",
    "    target_df = df.ffill(axis=1).ffill(axis=0)\n",
    "    table_df_list.append(target_df)\n",
    "# target_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ccf72",
   "metadata": {},
   "source": [
    "## Azure DI .md ì „ì²˜ë¦¬\n",
    "- â–  bullet ì˜ˆì™¸ ë°ì´í„° ì „ì²˜ë¦¬ (ì œëª©ì— ë¶€ì—° ì„¤ëª… ìˆëŠ” ê²½ìš°, ì¤„ë°”ê¿ˆ ì•ˆ ëœ ê²½ìš° ë“±)\n",
    "- â–  ê³„ì¸µêµ¬ì¡° ì‚­ì œ\n",
    "- ê³µë°± í…Œì´ë¸” êµì²´ (from PyMuPDF4LLM) -> (ê°œë°œ ì¤‘)\n",
    "- html í…Œì´ë¸” -> í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ëœ í…Œì´ë¸” (from LLM) -> (í•¨ìˆ˜í™” ë° ë³‘í•© í•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown íŒŒì¼ ì½ê¸°\n",
    "with open(\"document_result.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    azure_md = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39c1d5",
   "metadata": {},
   "source": [
    "### 1) â–  bullet ì˜ˆì™¸ ë°ì´í„° ì „ì²˜ë¦¬, ê³„ì¸µêµ¬ì¡° ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefa3028",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'azure_md' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# í˜ì´ì§€ ë¶„í• : í˜ì´ì§€ ë²ˆí˜¸ì™€ ë‚´ìš©ì„ ê·¸ë£¹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m split_pages = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<!--\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*PageNumber=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([^\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]*)\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*-->\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mazure_md\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ë¶ˆí•„ìš”í•œ ì²˜ìŒ í•­ëª© ì œê±° (í˜ì´ì§€ ë²ˆí˜¸ ì• í…ìŠ¤íŠ¸)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split_pages \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_pages[\u001b[32m0\u001b[39m].strip():\n",
      "\u001b[31mNameError\u001b[39m: name 'azure_md' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# í˜ì´ì§€ ë¶„í• : í˜ì´ì§€ ë²ˆí˜¸ì™€ ë‚´ìš©ì„ ê·¸ë£¹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "split_pages = re.split(r'<!--\\s*PageNumber=\"([^\"]*)\"\\s*-->', azure_md)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì²˜ìŒ í•­ëª© ì œê±° (í˜ì´ì§€ ë²ˆí˜¸ ì• í…ìŠ¤íŠ¸)\n",
    "if split_pages and not split_pages[0].strip():\n",
    "    split_pages = split_pages[1:]\n",
    "\n",
    "# í˜ì´ì§€ ëª©ë¡ì„ (í˜ì´ì§€ë²ˆí˜¸, ë‚´ìš©) íŠœí”Œë¡œ ë¬¶ê¸°\n",
    "page_pairs = list(zip(split_pages[::2], split_pages[1::2]))\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° í˜ì´ì§€ ì¬ì¡°í•©\n",
    "restructured_pages = []\n",
    "\n",
    "for page_number, content in page_pairs:\n",
    "    lines = content.splitlines()\n",
    "    new_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"â– \" in line:\n",
    "            # ë™ì‘1: '#' ë“± ë§ˆí¬ë‹¤ìš´ ê³„ì¸µ êµ¬ì¡° ì œê±°\n",
    "            cleaned_line = re.sub(r'^[#>\\-\\*\\s]+', '', line)\n",
    "            # ë™ì‘2: â–  ì•ì— ê³µë°±ì´ ìˆë‹¤ë©´ \\nâ–  ì²˜ë¦¬\n",
    "            cleaned_line = re.sub(r'\\s*â– ', r'\\nâ– ', cleaned_line)\n",
    "            # ë™ì‘3: ':'ê°€ ìˆë‹¤ë©´ \\n: ì²˜ë¦¬\n",
    "            cleaned_line = re.sub(r'\\s* :\\s*', r'\\n:', cleaned_line)\n",
    "            # ë™ì‘4: \") \" â†’ \")\\n\"\n",
    "            cleaned_line = re.sub(r'\\)\\s+', r')\\n', cleaned_line)\n",
    "            new_lines.append(cleaned_line)\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    # ì „ì²˜ë¦¬ëœ í˜ì´ì§€ ë‚´ìš© ì¡°í•©\n",
    "    modified_content = '\\n'.join(new_lines)\n",
    "    # page_comment = f'<!-- PageNumber=\"{page_number.strip()}\" -->'\n",
    "    restructured_pages.append(f\"{page_number.strip()}\")\n",
    "\n",
    "# ìµœì¢… ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ\n",
    "# final_markdown = '\\n\\n<!-- PageBreak -->\\n\\n'.join(restructured_pages)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n--- Final Markdown ---\\n\")\n",
    "print(restructured_pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64558fb0",
   "metadata": {},
   "source": [
    "### 2) ê³µë°± í…Œì´ë¸” êµì²´ (from PyMuPDF4LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bafb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure DI .mdì—ì„œ ì—°ì¥í‘œ index ì°¾ê¸° (ë°˜ë¡€ ê°€ëŠ¥ì„± æœ‰)\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def split_pages(markdown_text):\n",
    "    pattern = r'<!-- PageNumber=\"- (\\d+) -\" -->\\s*<!-- PageBreak -->'\n",
    "    parts = re.split(pattern, markdown_text)\n",
    "    pages = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        page_num = int(parts[i]) + 1\n",
    "        content = parts[i+1].strip()\n",
    "        pages.append((page_num, content))\n",
    "    return pages\n",
    "\n",
    "def is_table_only(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    text = soup.get_text().strip()\n",
    "    # íƒœê·¸ ì¤‘ table ê´€ë ¨ íƒœê·¸ë§Œ ìˆëŠ”ì§€ í™•ì¸\n",
    "    allowed_tags = {'table', 'tr', 'td', 'th'}\n",
    "    all_tags = {tag.name for tag in soup.find_all()}\n",
    "    return (text == '') and all_tags.issubset(allowed_tags)\n",
    "\n",
    "def detect_table_transition(pages):\n",
    "    transitions = []\n",
    "    page_unit = []\n",
    "    for i in range(len(pages) - 1):\n",
    "        \n",
    "        curr_page_num, curr_content = pages[i]\n",
    "        next_page_num, next_content = pages[i + 1]\n",
    "\n",
    "        if curr_content.strip().endswith('</table>') and next_content.strip().startswith('<table>'): # í˜„ í˜ì´ì§€ê°€ </table>ë¡œ ëë‚˜ê³  ë‹¤ìŒ í˜ì´ì§€ê°€ <table>ë¡œ ì‹œì‘í•  ë•Œ\n",
    "            if curr_page_num in page_unit: # ì´ë¯¸ í˜ì´ì§€ê°€ page_unitì— ìˆë‹¤ë©´\n",
    "                if curr_content.strip().startswith('<table>') and curr_content.strip().endswith('</table>'):\n",
    "                    if len(curr_content.split('</table>')) > 2: # í˜„ì¬ í˜ì´ì§€ ì•ˆì— í‘œê°€ 2ê°œ ì´ìƒ\n",
    "                        transitions.append(page_unit) # í˜„ì¬ í˜ì´ì§€ê¹Œì§€ ì €ì¥ëœ unit ì €ì¥\n",
    "                        page_unit = []\n",
    "                        page_unit.append(curr_page_num) # í˜„ì¬ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ ì¹´ìš´íŠ¸\n",
    "                        page_unit.append(next_page_num)\n",
    "                    else:\n",
    "                        page_unit.append(next_page_num) # í•œ í˜ì´ì§€ ì „ì²´ê°€ í‘œ. ë‹¤ìŒ í˜ì´ì§€ë§Œ ì €ì¥\n",
    "                else:\n",
    "                    transitions.append(page_unit)\n",
    "                    page_unit = []\n",
    "                    page_unit.append(curr_page_num)\n",
    "            else:\n",
    "                if page_unit:\n",
    "                    transitions.append(page_unit)\n",
    "                    page_unit = []\n",
    "                page_unit.append(curr_page_num)\n",
    "                page_unit.append(next_page_num)\n",
    "                \n",
    "    transitions.append(page_unit)\n",
    "    return transitions\n",
    "\n",
    "def merge_transitions(transitions, pages_dict):\n",
    "    if not transitions:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    current_group = transitions[0]\n",
    "\n",
    "    for next_pair in transitions[1:]:\n",
    "        prev_last = current_group[-1]\n",
    "        next_first = next_pair[0]\n",
    "\n",
    "        if prev_last == next_first:\n",
    "            # ì¤‘ê°„ í˜ì´ì§€ê°€ table-onlyì´ë©´ ë³‘í•©\n",
    "            if is_table_only(pages_dict[prev_last]):\n",
    "                current_group.append(next_pair[1])\n",
    "            else:\n",
    "                merged.append(current_group)\n",
    "                current_group = next_pair\n",
    "        else:\n",
    "            merged.append(current_group)\n",
    "            current_group = next_pair\n",
    "\n",
    "    merged.append(current_group)\n",
    "    return merged\n",
    "\n",
    "def process_markdown_for_table_groups(markdown_text):\n",
    "    pages = split_pages(markdown_text)\n",
    "    pages_dict = dict(pages)\n",
    "    transitions = detect_table_transition(pages)\n",
    "    merged_groups = merge_transitions(transitions, pages_dict)\n",
    "    return merged_groups\n",
    "\n",
    "# with open(\"document_result.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     azure_md = file.read()\n",
    "\n",
    "extended_page_list = process_markdown_for_table_groups(azure_md)\n",
    "\n",
    "print(extended_page_list)  # [[3, 4, 5], [7, 8], [10, 11]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‘œ êµì²´í•˜ëŠ” ì½”ë“œ (PyMuPDF4LLM ì „ì²˜ë¦¬ ê³¼ì • ì´í›„ ì‚¬ìš©)\n",
    "# ì™„ì„± í›„ Final Markdown ì €ì¥\n",
    "import re\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° í˜ì´ì§€ ì¬ì¡°í•©\n",
    "pattern = re.compile(r'<table[\\s\\S]*?</table>', re.IGNORECASE)\n",
    "\n",
    "for i in range(len(extended_page_list)):\n",
    "    page_list = extended_page_list[i]\n",
    "    df = table_df_list[i]\n",
    "    new_html_table = df.to_html(index=False, escape=False)\n",
    "\n",
    "    # ì²« í˜ì´ì§€\n",
    "    first_page = page_list[0]\n",
    "    page_md = restructured_pages[first_page-1]\n",
    "    matches = list(pattern.finditer(page_md))\n",
    "    matched_table = matches[-1].group()\n",
    "    new_page_md = page_md.replace(matched_table, new_html_table)\n",
    "    restructured_pages[first_page-1] = new_page_md\n",
    "\n",
    "    # ë§ˆì§€ë§‰ í˜ì´ì§€ + ë‚˜ë¨¸ì§€\n",
    "    for i in range(1, len(page_list)):\n",
    "        page_md = restructured_pages[page_list[i]-1]\n",
    "        matches = list(pattern.finditer(page_md))\n",
    "        matched_table = matches[0].group()\n",
    "        new_page_md = page_md.replace(matched_table, '')\n",
    "        restructured_pages[page_list[i]-1] = new_page_md\n",
    "\n",
    "final_md = '\\n'.join(restructured_pages)\n",
    "\n",
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190d1a3",
   "metadata": {},
   "source": [
    "### 3) html í…Œì´ë¸” -> í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ëœ í…Œì´ë¸” (from LLM) -> (í•¨ìˆ˜í™” ë° ë³‘í•© í•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94366b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyterë‚˜ VS Codeì—ì„œ ì‹¤í–‰ ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •í•œ ë²„ì „\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def process_file(input_file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Process an input file through Azure AI to convert HTML tables to text.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "    deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    subscription_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    \n",
    "    if not all([endpoint, deployment, subscription_key]):\n",
    "        raise ValueError(\"í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: ENDPOINT_URL, DEPLOYMENT_NAME, AZURE_OPENAI_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=subscription_key,\n",
    "        api_version=\"2024-05-01-preview\",\n",
    "    )\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        input_content = file.read()\n",
    "    \n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"ë„ˆëŠ” HTML í…Œì´ë¸”ì„ ì½ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ í˜• í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í…ìŠ¤íŠ¸ ë³€í™˜ ì—”ì§„ì´ë‹¤.\\n\\nì…ë ¥ ë°ì´í„°ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ì™€ HTML ì½”ë“œê°€ í˜¼í•©ëœ ë¬¸ì„œì´ë©°, ì´ ì¤‘ HTML í…Œì´ë¸”(`<table>`) í˜•ì‹ìœ¼ë¡œ ì‘ì„±ëœ í‘œë§Œì„ ê°ì§€í•˜ì—¬ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ **ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸**ë¡œ ë³€í™˜í•˜ë¼. í‘œ ì™¸ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” **ì ˆëŒ€ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤**. \\n\\nì¶œë ¥ëœ í…ìŠ¤íŠ¸ëŠ” ì•„ë˜ì˜ ê¸°ì¤€ì„ ëª¨ë‘ ë”°ë¼ì•¼ í•œë‹¤:\\n\\n1. í‘œì˜ ê³„ì¸µ êµ¬ì¡°, ì œëª©, ì…€ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ íŒŒì•…í•˜ì—¬ ìì—°ì–´ë¡œ ê¸°ìˆ í•œë‹¤.\\n2. ì…€ì´ ë³‘í•©ëœ ê²½ìš° (`rowspan`, `colspan`)ì—ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë‚´ìš©ì„ í†µí•©í•˜ì—¬ í’€ì–´ì„œ ì„¤ëª…í•œë‹¤.\\n3. í‘œ ì•ˆì— ë˜ ë‹¤ë¥¸ í‘œê°€ ì¤‘ì²©ë˜ì–´ ìˆëŠ” ê²½ìš°ì—ë„ ê° í‘œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•œë‹¤.\\n4. ë¹ˆ ì¹¸ì´ ìˆëŠ” ê²½ìš°, ë‚´ìš©ì„ ìœ ì¶”í•˜ì§€ ì•Šê³  \\\"(ë¹ˆì¹¸)\\\" ë˜ëŠ” \\\"í•´ë‹¹ ì—†ìŒ\\\" ë“±ìœ¼ë¡œ ëª…í™•í•˜ê²Œ í‘œê¸°í•œë‹¤.\\n5. í•­ëª© ê°„ êµ¬ë¶„ì€ \\\"â– \\\", \\\"1.\\\", \\\"-\\\" ë“±ì„ ì‚¬ìš©í•˜ì—¬ ëª…í™•íˆ êµ¬ë¶„í•˜ê³ , ê³„ì¸µì ìœ¼ë¡œ ì •ë¦¬í•œë‹¤.\\n6. ê²°ê³¼ í…ìŠ¤íŠ¸ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ë˜ê²Œ ì—°ê²°ë˜ì–´ì•¼ í•˜ë©°, ì›ë˜ ë¬¸ì„œì˜ íë¦„ê³¼ ì—°ê²°ë˜ë„ë¡ ì´ì–´ì ¸ì•¼ í•œë‹¤.\\n7. HTML íƒœê·¸ê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì˜ì—­ì€ ì ˆëŒ€ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¬êµ¬ì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.\\n8. ê²°ê³¼ëŠ” ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ê°€ë…ì„±ì„ ê°–ì¶°ì•¼ í•˜ë©°, í‘œë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì€ ê³µì‹ ë¬¸ì„œë‚˜ ê³„ì•½ì„œ ìŠ¤íƒ€ì¼ì²˜ëŸ¼ ëª…ë£Œí•˜ê³  ë‹¨ì •í•˜ê²Œ ì‘ì„±í•œë‹¤.\\n\\nì˜ˆì™¸ë‚˜ ì• ë§¤í•œ êµ¬ì¡°ê°€ ìˆì–´ë„ ìµœëŒ€í•œ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ì—¬ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ë¼.\\n\\nì…ë ¥ í˜•ì‹ ì˜ˆì‹œ:\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸)\\n<table>...</table>\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê³„ì†)\\n\\nì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\\n(ë³¸ë¬¸ í…ìŠ¤íŠ¸)\\nâ–  í•­ëª©ëª…  \\n- ë‚´ìš©1  \\n- ë‚´ìš©2  \\nì´ì œ ì•„ë˜ì— ì…ë ¥ëœ ë¬¸ì„œ ë‚´ HTML í…Œì´ë¸”ì„ ìœ„ ê¸°ì¤€ì— ë”°ë¼ ì„œìˆ í˜• í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ë¼.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": input_content\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=chat_prompt,\n",
    "        max_tokens=1500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    processed_text = completion.choices[0].message.content\n",
    "    \n",
    "    if not output_file_path:\n",
    "        base, _ = os.path.splitext(input_file_path)\n",
    "        output_file_path = f\"{base}_processed.txt\"\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_text)\n",
    "    \n",
    "    print(f\"âœ… ë³€í™˜ ì™„ë£Œ: {output_file_path}\")\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "#=======================================================================\n",
    "\n",
    "# ğŸ”½ ì—¬ê¸°ì„œ input/output ê²½ë¡œ ì§€ì •í•´ì„œ ì‹¤í–‰\n",
    "input_path = \"document_result.md\"  # â† ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "output_path = \"result.txt\"  # ë˜ëŠ” \"ê²°ê³¼ì €ì¥íŒŒì¼.txt\"\n",
    "\n",
    "process_file(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
