{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ğŸ”‘ .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "embedding_api_key = os.getenv(\"Embedding_API_KEY\")\n",
    "embedding_endpoint = os.getenv(\"Embedding_ENDPOINT\")\n",
    "gpt_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gpt_endpoint = os.getenv('OPENAI_ENDPOINT')\n",
    "BLOB_CONN_STR = os.getenv('BLOB_CONN_STR')\n",
    "DI_ENDPOINT = os.getenv('DI_ENDPOINT')\n",
    "DI_API_KEY = os.getenv('DI_API_KEY')\n",
    "\n",
    "# ğŸ“ ê²½ë¡œ ì„¤ì •\n",
    "BLOB_CONTAINER_NAME = \"pdf-container\"\n",
    "PDF_FOLDER = r\"E:\\work\\MS_project_2\\code\\í…Œì´ë¸”ì²˜ë¦¬o\\data\\pdfs/d\"\n",
    "MD_FOLDER = r\"E:\\work\\MS_project_2\\code\\í…Œì´ë¸”ì²˜ë¦¬o\\data\\markdowns\"\n",
    "os.makedirs(MD_FOLDER, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "blob_service = BlobServiceClient.from_connection_string(BLOB_CONN_STR)\n",
    "container_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "di_client = DocumentIntelligenceClient(endpoint=DI_ENDPOINT, credential=AzureKeyCredential(DI_API_KEY))\n",
    "\n",
    "\n",
    "def upload_pdf_to_blob(pdf_path: str, blob_name: str) -> str:\n",
    "    \"\"\"PDFë¥¼ Blobì— ì—…ë¡œë“œí•˜ê³  SAS URL ë°˜í™˜\"\"\"\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_service.account_name,\n",
    "        container_name=BLOB_CONTAINER_NAME,\n",
    "        blob_name=blob_name,\n",
    "        account_key=blob_service.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(minutes=15)\n",
    "    )\n",
    "\n",
    "    return f\"{blob_client.url}?{sas_token}\"\n",
    "\n",
    "\n",
    "def analyze_pdf_to_markdown(sas_url: str) -> str:\n",
    "    \"\"\"Document Intelligenceë¥¼ ì‚¬ìš©í•´ Markdownìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    poller = di_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-layout\",\n",
    "        body=AnalyzeDocumentRequest(url_source=sas_url),\n",
    "        output_content_format='markdown'\n",
    "    )\n",
    "    result = poller.result()\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def request_gpt(prompt: str) -> str:\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': gpt_api_key\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    'ë„ˆëŠ” HTML í…Œì´ë¸”ì„ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•´.  '\n",
    "                    'í•­ëª©ê³¼ ê°’ì„ \"êµ¬ë¶„: ë‚´ìš©\" ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ë§ê³ , ì›ë˜ í…Œì´ë¸”ì—ì„œ ì“°ì¸ í•­ëª©ëª…ì„ ê·¸ëŒ€ë¡œ keyë¡œ ì‚¬ìš©í•´.  '\n",
    "                    'ì˜ˆ: \"ì„ëŒ€ì¡°ê±´ : ë‚´ìš©\", \"ì„ëŒ€ë³´ì¦ê¸ˆ-ì›”ì„ëŒ€ë£Œ ì „í™˜ : ë‚´ìš©\"ì²˜ëŸ¼.  '\n",
    "                    'í•­ëª©ì´ ë°˜ë³µë˜ëŠ” ê²½ìš°ì—ëŠ” êµ¬ë¶„ìë¥¼ ë¶™ì—¬ì„œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì¤˜.  '\n",
    "                    'ë¶ˆí•„ìš”í•œ ìš”ì•½ì´ë‚˜ ë„ì…ë¶€ ì—†ì´ í‘œì˜ í•µì‹¬ ë‚´ìš©ë§Œ ë³€í™˜í•´ì¤˜.'\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    response = requests.post(gpt_endpoint, headers=headers, json=body)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(\"âŒ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code, response.text)\n",
    "        return \"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "def convert_md_tables_with_llm_parallel(md_text: str, max_workers=5) -> str:\n",
    "    soup = BeautifulSoup(md_text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_strs = [str(table) for table in tables]\n",
    "    unique_tables = list(set(table_strs))\n",
    "    table_to_text = {}\n",
    "\n",
    "    def process_table(table_html):\n",
    "        prompt = (\n",
    "            f\"ë‹¤ìŒ HTML í…Œì´ë¸”ì˜ ë‚´ìš©ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë³€í™˜í•´ì¤˜.\\n\\n{table_html}\"\n",
    "        )\n",
    "        result = request_gpt(prompt)\n",
    "        return table_html, result\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_table, tbl) for tbl in unique_tables]\n",
    "        for future in as_completed(futures):\n",
    "            tbl_html, gpt_result = future.result()\n",
    "            table_to_text[tbl_html] = gpt_result\n",
    "\n",
    "    for original_table in table_strs:\n",
    "        if original_table in table_to_text:\n",
    "            md_text = md_text.replace(original_table, table_to_text[original_table])\n",
    "\n",
    "    return md_text\n",
    "\n",
    "\n",
    "def preprocess_markdown_headers(md_text: str) -> str:\n",
    "    md_text = re.sub(r'^(#{1,6}\\s*â– ?\\s*[^:\\n]+):\\s*(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    md_text = re.sub(r'^(â– \\s*\\([^)]+\\))\\s+(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    return md_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì²˜ë¦¬í•  PDF íŒŒì¼ ìˆ˜: 2\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì„œìš¸ì§€ì—­ë³¸ë¶€ ì²­ë…„ ë§¤ì…ì„ëŒ€ì£¼íƒ\n",
      "âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\n",
      "âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\n",
      "âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: E:\\work\\MS_project_2\\code\\í…Œì´ë¸”ì²˜ë¦¬o\\data\\markdowns\\ì„œìš¸ì§€ì—­ë³¸ë¶€ ì²­ë…„ ë§¤ì…ì„ëŒ€ì£¼íƒ.md\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì•„ì¸ ìŠ¤í…Œì´ì˜ë“±í¬_ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\n",
      "âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\n",
      "âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: E:\\work\\MS_project_2\\code\\í…Œì´ë¸”ì²˜ë¦¬o\\data\\markdowns\\ì•„ì¸ ìŠ¤í…Œì´ì˜ë“±í¬_ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸.md\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì „ì²´ ì²˜ë¦¬ ë£¨í”„\n",
    "pdf_files = glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
    "print(f\"ğŸ” ì²˜ë¦¬í•  PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    blob_name = f\"{filename}.pdf\"\n",
    "    md_path = os.path.join(MD_FOLDER, f\"{filename}.md\")\n",
    "\n",
    "    print(f\"\\nğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}\")\n",
    "\n",
    "    # 1. ì—…ë¡œë“œ ë° SAS URL ìƒì„±\n",
    "    sas_url = upload_pdf_to_blob(pdf_path, blob_name)\n",
    "    print(\"âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\")\n",
    "\n",
    "    # 2. Markdown ë³€í™˜\n",
    "    md_content = analyze_pdf_to_markdown(sas_url)\n",
    "    print(\"âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\")\n",
    "\n",
    "    # 3. GPT í…Œì´ë¸” ë³€í™˜\n",
    "    md_with_tables = convert_md_tables_with_llm_parallel(md_content)\n",
    "    print(\"âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\")\n",
    "\n",
    "    # 4. í—¤ë” ì „ì²˜ë¦¬\n",
    "    final_md = preprocess_markdown_headers(md_with_tables)\n",
    "\n",
    "    # 5. ì €ì¥\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_md)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {md_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì´ Markdown íŒŒì¼ ìˆ˜: 8\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (ëŒ€ì „ì¶©ë‚¨)25ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€_í‘œì¤€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (ì •ì •ê³µê³ ë¬¸)25ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€_í‘œì¤€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: 2025ë…„ 1ì°¨ ëŒ€êµ¬ê²½ë¶ ì²­ë…„ë§¤ì…ì„ëŒ€ ì…ì£¼ì ëª¨ì§‘ ê³µê³ ë¬¸\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: 2025ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸(ê´‘ì£¼ì „ë‚¨)\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: 25ë…„ 1ì°¨ ì²­ë…„ë§¤ì…ì„ëŒ€ ì…ì£¼ì ëª¨ì§‘ ê³µê³ ë¬¸(ê°•ì›ì§€ì—­ë³¸ë¶€)\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: 25ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: ì„œìš¸ì§€ì—­ë³¸ë¶€ ì²­ë…„ ë§¤ì…ì„ëŒ€ì£¼íƒ\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: ì•„ì¸ ìŠ¤í…Œì´ì˜ë“±í¬_ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "\n",
      "âœ… ì „ì²´ ì²­í¬ ìˆ˜: 835\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ğŸ“ ë§ˆí¬ë‹¤ìš´ í´ë” ê²½ë¡œ (raw stringìœ¼ë¡œ ê²½ë¡œ ì‘ì„±)\n",
    "MARKDOWN_FOLDER = r\"E:\\work\\MS_project_2\\code\\í…Œì´ë¸”ì²˜ë¦¬o\\data\\markdowns\"\n",
    "\n",
    "# âœ… ë¶„í•  ë„êµ¬ ì •ì˜\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"section\"),\n",
    "    (\"##\", \"subsection\"),\n",
    "    (\"###\", \"subsubsection\"),\n",
    "    (\"â– \", \"bullet\"),\n",
    "    (\"â€»\", \"bullet\"),\n",
    "    (\"â–ª\", \"subbullet\"),\n",
    "    (\"âœ”\", \"check\")\n",
    "])\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# âœ… ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "all_docs = []\n",
    "\n",
    "# âœ… ëª¨ë“  .md íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "md_files = glob(os.path.join(MARKDOWN_FOLDER, \"*.md\"))\n",
    "print(f\"ğŸ“„ ì´ Markdown íŒŒì¼ ìˆ˜: {len(md_files)}\")\n",
    "\n",
    "# âœ… ê° íŒŒì¼ ì²˜ë¦¬\n",
    "for md_path in md_files:\n",
    "    filename = os.path.splitext(os.path.basename(md_path))[0]  # í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…\n",
    "\n",
    "    print(f\"\\nğŸš€ ì²˜ë¦¬ ì¤‘: {filename}\")\n",
    "\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_text = f.read()\n",
    "\n",
    "    # 1. Markdown í—¤ë” ê¸°ì¤€ ë¶„í• \n",
    "    header_docs = header_splitter.split_text(md_text)\n",
    "\n",
    "    # 2. ê° ë¬¸ì„œì— íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "    for doc in header_docs:\n",
    "        doc.metadata[\"source\"] = filename\n",
    "\n",
    "    # 3. RecursiveCharacterTextSplitterë¡œ chunk ë¶„í• \n",
    "    for doc in header_docs:\n",
    "        sub_docs = recursive_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for chunk in sub_docs:\n",
    "            all_docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata=doc.metadata  # section, subsection, bullet, source í¬í•¨\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ ì²­í¬ ìˆ˜: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bullet': 'ê³µë™ê±°ì£¼í˜•ì„ì„ ì¸ì§€í•˜ì§€ ëª»í•˜ê³  ì‹ ì²­í•œ í›„ ê³„ì•½ì„ í¬ê¸°í•˜ëŠ” ì‚¬ë¡€ê°€ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.', 'section': '2 ì…ì£¼ìê²© ë° ì…ì£¼ì ì„ ì •ê¸°ì¤€', 'subsection': 'â–  ì…ì£¼ìˆœìœ„', 'source': '(ëŒ€ì „ì¶©ë‚¨)25ë…„1ì°¨ì²­ë…„ë§¤ì…ì„ëŒ€_í‘œì¤€ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸'}\n",
      "=========================\n",
      "2ìˆœìœ„ëŠ” ë³¸ì¸ê³¼ ë¶€ëª¨ì˜ ì›”í‰ê·  ì†Œë“ì´ ì „ë…„ë„ ë„ì‹œê·¼ë¡œì ê°€êµ¬ì›ìˆ˜ë³„ ê°€êµ¬ë‹¹ ì›”í‰ê· ì†Œë“ì˜ 100% ì´í•˜ì´ë©°, ë³¸ì¸ê³¼ ë¶€ëª¨ì˜ ìì‚°ì´ êµ­ë¯¼ì„ëŒ€ì£¼íƒ ìì‚°ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ìì…ë‹ˆë‹¤.  \n",
      "3ìˆœìœ„ëŠ” ë³¸ì¸ì˜ ì›”í‰ê·  ì†Œë“ì´ ì „ë…„ë„ ë„ì‹œê·¼ë¡œì 1ì¸ ê°€êµ¬ ì›”í‰ê· ì†Œë“ 100% ì´í•˜ì´ë©°, í–‰ë³µì£¼íƒ(ì²­ë…„) ìì‚°ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ìì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(all_docs[11].metadata)\n",
    "print('=========================')\n",
    "print(all_docs[11].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "embedding_api_version = \"2024-02-15-preview\"\n",
    "embedding_deployment = \"text-embedding-3-small\"\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Azure AI Search ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ ì—…ë¡œë“œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 835/835 [04:58<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²´ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchField, SearchFieldDataType,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "# âœ… 1. API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸\n",
    "#ai_search_index_name = \"new_index\"\n",
    "ai_search_index_name = \"add_new_index\"\n",
    "\n",
    "ai_search_endpoint = os.getenv('AI_Search_ENDPOINT')\n",
    "ai_search_api_key = os.getenv('AI_Search_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "# âœ… 2. ì„ë² ë”© ì„¤ì •\n",
    "embedding_deployment = \"text-embedding-3-small\"\n",
    "embedding_api_version = \"2024-02-15-preview\"\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")\n",
    "\n",
    "# âœ… 3. ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "embedding_dim = 1536\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(name=\"source\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"section\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsubsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"bullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subbullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"check\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(\n",
    "        name=\"embedding\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=embedding_dim,\n",
    "        vector_search_profile_name=\"default\"\n",
    "    )\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"default\", algorithm_configuration_name=\"my-algorithm\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithm\", kind=VectorSearchAlgorithmKind.HNSW)]\n",
    ")\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=ai_search_index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search\n",
    ")\n",
    "\n",
    "# âœ… 4. ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ìƒì„±\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "if ai_search_index_name in [i.name for i in index_client.list_indexes()]:\n",
    "    index_client.delete_index(ai_search_index_name)\n",
    "    print(\"ğŸ—‘ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ\")\n",
    "\n",
    "index_client.create_index(index)\n",
    "print(\"âœ… Azure AI Search ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# âœ… 5. ë²¡í„° ë°ì´í„° ì—…ë¡œë“œ\n",
    "# ğŸ‘‰ ì—¬ê¸°ì„œ all_docsëŠ” ì´ë¯¸ ë§Œë“¤ì–´ì§„ Document ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •\n",
    "\n",
    "# ì˜ˆ: all_docs = [Document(page_content=..., metadata={...}), ...]\n",
    "\n",
    "search_client = SearchClient(endpoint=ai_search_endpoint, index_name=ai_search_index_name, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for doc in tqdm(all_docs, desc=\"ğŸ“¤ ì—…ë¡œë“œ ì¤‘\"):\n",
    "    vector = embedding.embed_query(doc.page_content)\n",
    "\n",
    "    record = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": doc.page_content,\n",
    "        \"embedding\": vector,\n",
    "        \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "        \"section\": doc.metadata.get(\"section\", \"\"),\n",
    "        \"subsection\": doc.metadata.get(\"subsection\", \"\"),\n",
    "        \"subsubsection\": doc.metadata.get(\"subsubsection\", \"\"),\n",
    "        \"bullet\": doc.metadata.get(\"bullet\", \"\"),\n",
    "        \"subbullet\": doc.metadata.get(\"subbullet\", \"\"),\n",
    "        \"check\": doc.metadata.get(\"check\", \"\")\n",
    "    }\n",
    "\n",
    "    batch.append(record)\n",
    "\n",
    "    if len(batch) >= BATCH_SIZE:\n",
    "        search_client.upload_documents(documents=batch)\n",
    "        batch = []\n",
    "\n",
    "# ë‚¨ì€ ë°ì´í„° ì—…ë¡œë“œ\n",
    "if batch:\n",
    "    search_client.upload_documents(documents=batch)\n",
    "\n",
    "print(\"âœ… ì „ì²´ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
