{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# 🔑 .env 로드\n",
    "load_dotenv()\n",
    "embedding_api_key = os.getenv(\"Embedding_API_KEY\")\n",
    "embedding_endpoint = os.getenv(\"Embedding_ENDPOINT\")\n",
    "gpt_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gpt_endpoint = os.getenv('OPENAI_ENDPOINT')\n",
    "BLOB_CONN_STR = os.getenv('BLOB_CONN_STR')\n",
    "DI_ENDPOINT = os.getenv('DI_ENDPOINT')\n",
    "DI_API_KEY = os.getenv('DI_API_KEY')\n",
    "\n",
    "# 📁 경로 설정\n",
    "BLOB_CONTAINER_NAME = #constainer name\n",
    "PDF_FOLDER =   # PDF 파일이 저장될 폴더\n",
    "MD_FOLDER =   # 마크다운 파일이 저장될 폴더\n",
    "os.makedirs(MD_FOLDER, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Tables into Words for Better Understanding for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ 클라이언트 초기화\n",
    "blob_service = BlobServiceClient.from_connection_string(BLOB_CONN_STR)\n",
    "container_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "di_client = DocumentIntelligenceClient(endpoint=DI_ENDPOINT, credential=AzureKeyCredential(DI_API_KEY))\n",
    "\n",
    "\n",
    "def upload_pdf_to_blob(pdf_path: str, blob_name: str) -> str:\n",
    "    \"\"\"PDF를 Blob에 업로드하고 SAS URL 반환\"\"\"\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_service.account_name,\n",
    "        container_name=BLOB_CONTAINER_NAME,\n",
    "        blob_name=blob_name,\n",
    "        account_key=blob_service.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(minutes=15)\n",
    "    )\n",
    "\n",
    "    return f\"{blob_client.url}?{sas_token}\"\n",
    "\n",
    "\n",
    "def analyze_pdf_to_markdown(sas_url: str) -> str:\n",
    "    \"\"\"Document Intelligence를 사용해 Markdown으로 변환\"\"\"\n",
    "    poller = di_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-layout\",\n",
    "        body=AnalyzeDocumentRequest(url_source=sas_url),\n",
    "        output_content_format='markdown'\n",
    "    )\n",
    "    result = poller.result()\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def request_gpt(prompt: str) -> str:\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': gpt_api_key\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    '너는 HTML 테이블을 사람이 이해할 수 있도록 자연어 문장으로 변환해.  '\n",
    "                    '항목과 값을 \"구분: 내용\" 식으로 나누지 말고, 원래 테이블에서 쓰인 항목명을 그대로 key로 사용해.  '\n",
    "                    '예: \"임대조건 : 내용\", \"임대보증금-월임대료 전환 : 내용\"처럼.  '\n",
    "                    '항목이 반복되는 경우에는 구분자를 붙여서 명확하게 구분해줘.  '\n",
    "                    '불필요한 요약이나 도입부 없이 표의 핵심 내용만 변환해줘.'\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    response = requests.post(gpt_endpoint, headers=headers, json=body)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(\"❌ 요청 실패:\", response.status_code, response.text)\n",
    "        return \"⚠️ 오류가 발생했습니다.\"\n",
    "\n",
    "\n",
    "def convert_md_tables_with_llm_parallel(md_text: str, max_workers=5) -> str:\n",
    "    soup = BeautifulSoup(md_text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_strs = [str(table) for table in tables]\n",
    "    unique_tables = list(set(table_strs))\n",
    "    table_to_text = {}\n",
    "\n",
    "    def process_table(table_html):\n",
    "        prompt = (\n",
    "            f\"다음 HTML 테이블의 내용을 자연어 문장으로 간결하게 변환해줘.\\n\\n{table_html}\"\n",
    "        )\n",
    "        result = request_gpt(prompt)\n",
    "        return table_html, result\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_table, tbl) for tbl in unique_tables]\n",
    "        for future in as_completed(futures):\n",
    "            tbl_html, gpt_result = future.result()\n",
    "            table_to_text[tbl_html] = gpt_result\n",
    "\n",
    "    for original_table in table_strs:\n",
    "        if original_table in table_to_text:\n",
    "            md_text = md_text.replace(original_table, table_to_text[original_table])\n",
    "\n",
    "    return md_text\n",
    "\n",
    "\n",
    "def preprocess_markdown_headers(md_text: str) -> str:\n",
    "    md_text = re.sub(r'^(#{1,6}\\s*■?\\s*[^:\\n]+):\\s*(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    md_text = re.sub(r'^(■\\s*\\([^)]+\\))\\s+(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    return md_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 처리할 PDF 파일 수: 2\n",
      "\n",
      "📄 처리 중: 붙임1. (25.03.28.)청년매입임대 입주자 모집공고문_게시용\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blank\\AppData\\Local\\Temp\\ipykernel_4136\\1607090331.py:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  expiry=datetime.utcnow() + timedelta(minutes=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blob 업로드 및 SAS URL 완료\n",
      "✅ Document Intelligence 분석 완료\n",
      "✅ GPT 테이블 변환 완료\n",
      "✅ 저장 완료: C:\\Users\\blank\\OneDrive\\바탕 화면\\2차 프로젝트 레포\\홍원님 모델\\markdowns\\붙임1. (25.03.28.)청년매입임대 입주자 모집공고문_게시용.md\n",
      "\n",
      "📄 처리 중: 아츠스테이영등포_입주자모집공고문\n",
      "✅ Blob 업로드 및 SAS URL 완료\n",
      "✅ Document Intelligence 분석 완료\n",
      "✅ GPT 테이블 변환 완료\n",
      "✅ 저장 완료: C:\\Users\\blank\\OneDrive\\바탕 화면\\2차 프로젝트 레포\\홍원님 모델\\markdowns\\아츠스테이영등포_입주자모집공고문.md\n"
     ]
    }
   ],
   "source": [
    "# ✅ 전체 처리 루프\n",
    "pdf_files = glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
    "print(f\"🔍 처리할 PDF 파일 수: {len(pdf_files)}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    blob_name = f\"{filename}.pdf\"\n",
    "    md_path = os.path.join(MD_FOLDER, f\"{filename}.md\")\n",
    "\n",
    "    print(f\"\\n📄 처리 중: {filename}\")\n",
    "\n",
    "    # 1. 업로드 및 SAS URL 생성\n",
    "    sas_url = upload_pdf_to_blob(pdf_path, blob_name)\n",
    "    print(\"✅ Blob 업로드 및 SAS URL 완료\")\n",
    "\n",
    "    # 2. Markdown 변환\n",
    "    md_content = analyze_pdf_to_markdown(sas_url)\n",
    "    print(\"✅ Document Intelligence 분석 완료\")\n",
    "\n",
    "    # 3. GPT 테이블 변환\n",
    "    md_with_tables = convert_md_tables_with_llm_parallel(md_content)\n",
    "    print(\"✅ GPT 테이블 변환 완료\")\n",
    "\n",
    "    # 4. 헤더 전처리\n",
    "    final_md = preprocess_markdown_headers(md_with_tables)\n",
    "\n",
    "    # 5. 저장\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_md)\n",
    "    print(f\"✅ 저장 완료: {md_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining with the Vocab Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 파일이 'mixed_md' 폴더에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def merge_with_glossary(glossary_path, input_folder, output_folder):\n",
    "    # 용어집 읽기\n",
    "    with open(glossary_path, 'r', encoding='utf-8') as glossary_file:\n",
    "        glossary_content = glossary_file.read().strip()\n",
    "\n",
    "    # 출력 폴더가 없으면 생성\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 폴더 내의 모든 md 파일 처리\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.md') and filename != os.path.basename(glossary_path):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                file_content = f.read().strip()\n",
    "\n",
    "            # 구분선으로 구분해서 합치기\n",
    "            separator = \"\\n\\n---\\n\\n\"\n",
    "            merged_content = file_content + separator + glossary_content\n",
    "\n",
    "            # 출력 경로\n",
    "            output_path = os.path.join(output_folder, f\"{filename}\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(merged_content)\n",
    "\n",
    "    print(f\"모든 파일이 '{output_folder}' 폴더에 저장되었습니다.\")\n",
    "\n",
    "# 사용 예시\n",
    "merge_with_glossary(\n",
    "    glossary_path=\"용어집.md\",\n",
    "    input_folder=\"processed_gpt\",   # 여기에 md 파일들이 들어있는 폴더 경로 입력\n",
    "    output_folder=\"mixed_md\"      # 결과 저장 폴더\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# mixed_md 디렉토리 경로 설정\n",
    "mixed_md_dir = 'mixed_md'\n",
    "\n",
    "# fin_으로 시작하는 모든 md 파일 찾기\n",
    "fin_files = glob.glob(os.path.join(mixed_md_dir, 'fin_*.md'))\n",
    "\n",
    "# 각 파일에 대해 처리\n",
    "for old_path in fin_files:\n",
    "    # 파일 이름만 추출\n",
    "    file_name = os.path.basename(old_path)\n",
    "    \n",
    "    # fin_ 접두사 제거\n",
    "    new_file_name = file_name.replace('fin_', '', 1)\n",
    "    \n",
    "    # 새 파일 경로 생성\n",
    "    new_path = os.path.join(mixed_md_dir, new_file_name)\n",
    "    \n",
    "    try:\n",
    "        # 파일 이름 변경\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"{file_name} → {new_file_name} 이름 변경 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 이름 변경 중 오류 발생: {e}\")\n",
    "\n",
    "print(\"모든 파일 처리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Spliiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 총 Markdown 파일 수: 17\n",
      "\n",
      "🚀 처리 중: (GH)[주택] 25년 1차 기존주택 등 매입임대주택 예비입주자 모집공고(경기도 27개 시군)\n",
      "\n",
      "🚀 처리 중: (GH)[주택] 25년 1차 청년매입임대주택 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)(인천) 청년.신혼부부 매입임대리츠주택 입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)2025년 청년 전세임대 1순위 입주자 수시모집\n",
      "\n",
      "🚀 처리 중: (LH)[경기북부] 25년 1차 청년매입임대 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)[서울지역본부] 25년 1차 청년 매입임대주택 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)[정정공고][경기남부] 25년 1차 청년 매입임대주택 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)[정정공고][서울지역본부] 고령자 매입임대주택 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)[정정공고][서울지역본부] 기존주택 등 매입임대주택 예비입주자 모집공고\n",
      "\n",
      "🚀 처리 중: (LH)[정정공고]구리갈매 이스트힐 10년 공공임대주택 예비입주자 모집 공고\n",
      "\n",
      "🚀 처리 중: (LH)[정정공고]전북 남부권 지역 국민임대주택 예비입주자 모집\n",
      "\n",
      "🚀 처리 중: (LH)관악봉천 H-1ㆍ2ㆍ3BL 행복주택 입주자격완화 추가모집\n",
      "\n",
      "🚀 처리 중: (LH)군포시 영구임대주택 예비입주자 모집\n",
      "\n",
      "🚀 처리 중: (LH)김포한강 Ac-05블록 10년 공공임대주택(리츠) 예비입주자 모집 공고\n",
      "\n",
      "🚀 처리 중: (LH)청년신혼부부매입임대리츠-2502-경기남부\n",
      "\n",
      "🚀 처리 중: (LH)청년신혼부부매입임대리츠-2502-경기북부\n",
      "\n",
      "🚀 처리 중: (LH)화성동탄2 10년 분양전환공공임대주택(리츠) 예비입주자모집공고('25.03)\n",
      "\n",
      "✅ 전체 청크 수: 4933\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 📁 마크다운 폴더 경로 (raw string으로 경로 작성)\n",
    "MARKDOWN_FOLDER = # MD_FOLDER\n",
    "\n",
    "# ✅ 분할 도구 정의\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"section\"),\n",
    "    (\"##\", \"subsection\"),\n",
    "    (\"###\", \"subsubsection\"),\n",
    "    (\"■\", \"bullet\"),\n",
    "    (\"※\", \"bullet\"),\n",
    "    (\"▪\", \"subbullet\"),\n",
    "    (\"✔\", \"check\")\n",
    "])\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# ✅ 전체 문서 리스트\n",
    "all_docs = []\n",
    "\n",
    "# ✅ 모든 .md 파일 가져오기\n",
    "md_files = glob(os.path.join(MARKDOWN_FOLDER, \"*.md\"))\n",
    "print(f\"📄 총 Markdown 파일 수: {len(md_files)}\")\n",
    "\n",
    "# ✅ 각 파일 처리\n",
    "for md_path in md_files:\n",
    "    filename = os.path.splitext(os.path.basename(md_path))[0]  # 확장자 없는 파일명\n",
    "\n",
    "    print(f\"\\n🚀 처리 중: {filename}\")\n",
    "\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_text = f.read()\n",
    "\n",
    "    # 1. Markdown 헤더 기준 분할\n",
    "    header_docs = header_splitter.split_text(md_text)\n",
    "\n",
    "    # 2. 각 문서에 파일명 메타데이터 추가\n",
    "    for doc in header_docs:\n",
    "        doc.metadata[\"source\"] = filename\n",
    "\n",
    "    # 3. RecursiveCharacterTextSplitter로 chunk 분할\n",
    "    for doc in header_docs:\n",
    "        sub_docs = recursive_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for chunk in sub_docs:\n",
    "            all_docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata=doc.metadata  # section, subsection, bullet, source 포함\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(f\"\\n✅ 전체 청크 수: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bullet': '희망 시 가구원 수보다 작은 규모의 주택 신청 가능', 'source': '(GH)[주택]\\xa025년 1차 기존주택 등 매입임대주택 예비입주자 모집공고(경기도 27개 시군)'}\n",
      "=========================\n",
      "(예시: 5인 가구의 경우 3형 대상이나, 신청자 희망 시 1형이나 2형으로 신청 가능)\n"
     ]
    }
   ],
   "source": [
    "print(all_docs[11].metadata)\n",
    "print('=========================')\n",
    "print(all_docs[11].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading into Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "embedding_api_version = #embedding_api_version\n",
    "embedding_deployment = #embedding_deployment\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Existing indexes: ['add_new_index', 'new_index', 'new_pdf_all_index', 'pdf_1_index', 'pdf_all_index', 'pdf_with_vocab_new_index']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "load_dotenv()\n",
    "\n",
    "# 🔑 Azure Cognitive Search 키 확인용 코드드\n",
    "ai_search_api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "ai_search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=AzureKeyCredential(ai_search_api_key))\n",
    "indexes = [i.name for i in index_client.list_indexes()]\n",
    "print(\"📦 Existing indexes:\", indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Azure AI Search 인덱스 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📤 업로드 중:   2%|▏         | 103/4933 [00:47<32:39,  2.47it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchField, SearchFieldDataType,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "# ✅ 1. API 키 및 엔드포인트\n",
    "ai_search_index_name = # \"your-index-name\" \n",
    "\n",
    "ai_search_endpoint = ai_search_endpoint\n",
    "ai_search_api_key = ai_search_api_key\n",
    "\n",
    "\n",
    "# ✅ 2. 임베딩 설정\n",
    "embedding_deployment = # embedding_deployment\n",
    "embedding_api_version = # embedding_api_version\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")\n",
    "\n",
    "# ✅ 3. 인덱스 스키마 정의\n",
    "embedding_dim = 1536\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(name=\"source\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"section\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsubsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"bullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subbullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"check\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(\n",
    "        name=\"embedding\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=embedding_dim,\n",
    "        vector_search_profile_name=\"default\"\n",
    "    )\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"default\", algorithm_configuration_name=\"my-algorithm\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithm\", kind=VectorSearchAlgorithmKind.HNSW)]\n",
    ")\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=ai_search_index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search\n",
    ")\n",
    "\n",
    "# ✅ 4. 인덱스 초기화 및 생성\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "if ai_search_index_name in [i.name for i in index_client.list_indexes()]:\n",
    "    index_client.delete_index(ai_search_index_name)\n",
    "    print(\"🗑 기존 인덱스 삭제 완료\")\n",
    "\n",
    "index_client.create_index(index)\n",
    "print(\"✅ Azure AI Search 인덱스 생성 완료\")\n",
    "\n",
    "# ✅ 5. 벡터 데이터 업로드\n",
    "# 👉 여기서 all_docs는 이미 만들어진 Document 리스트라고 가정\n",
    "\n",
    "# 예: all_docs = [Document(page_content=..., metadata={...}), ...]\n",
    "\n",
    "search_client = SearchClient(endpoint=ai_search_endpoint, index_name=ai_search_index_name, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for doc in tqdm(all_docs, desc=\"📤 업로드 중\"):\n",
    "    vector = embedding.embed_query(doc.page_content)\n",
    "\n",
    "    record = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": doc.page_content,\n",
    "        \"embedding\": vector,\n",
    "        \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "        \"section\": doc.metadata.get(\"section\", \"\"),\n",
    "        \"subsection\": doc.metadata.get(\"subsection\", \"\"),\n",
    "        \"subsubsection\": doc.metadata.get(\"subsubsection\", \"\"),\n",
    "        \"bullet\": doc.metadata.get(\"bullet\", \"\"),\n",
    "        \"subbullet\": doc.metadata.get(\"subbullet\", \"\"),\n",
    "        \"check\": doc.metadata.get(\"check\", \"\")\n",
    "    }\n",
    "\n",
    "    batch.append(record)\n",
    "\n",
    "    if len(batch) >= BATCH_SIZE:\n",
    "        search_client.upload_documents(documents=batch)\n",
    "        batch = []\n",
    "\n",
    "# 남은 데이터 업로드\n",
    "if batch:\n",
    "    search_client.upload_documents(documents=batch)\n",
    "\n",
    "print(\"✅ 전체 문서 업로드 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
