{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "# ğŸ”‘ .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "embedding_api_key = os.getenv(\"Embedding_API_KEY\")\n",
    "embedding_endpoint = os.getenv(\"Embedding_ENDPOINT\")\n",
    "gpt_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gpt_endpoint = os.getenv('OPENAI_ENDPOINT')\n",
    "BLOB_CONN_STR = os.getenv('BLOB_CONN_STR')\n",
    "DI_ENDPOINT = os.getenv('DI_ENDPOINT')\n",
    "DI_API_KEY = os.getenv('DI_API_KEY')\n",
    "\n",
    "# ğŸ“ ê²½ë¡œ ì„¤ì •\n",
    "BLOB_CONTAINER_NAME = #constainer name\n",
    "PDF_FOLDER =   # PDF íŒŒì¼ì´ ì €ì¥ë  í´ë”\n",
    "MD_FOLDER =   # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì €ì¥ë  í´ë”\n",
    "os.makedirs(MD_FOLDER, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Tables into Words for Better Understanding for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "blob_service = BlobServiceClient.from_connection_string(BLOB_CONN_STR)\n",
    "container_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "di_client = DocumentIntelligenceClient(endpoint=DI_ENDPOINT, credential=AzureKeyCredential(DI_API_KEY))\n",
    "\n",
    "\n",
    "def upload_pdf_to_blob(pdf_path: str, blob_name: str) -> str:\n",
    "    \"\"\"PDFë¥¼ Blobì— ì—…ë¡œë“œí•˜ê³  SAS URL ë°˜í™˜\"\"\"\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_service.account_name,\n",
    "        container_name=BLOB_CONTAINER_NAME,\n",
    "        blob_name=blob_name,\n",
    "        account_key=blob_service.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(minutes=15)\n",
    "    )\n",
    "\n",
    "    return f\"{blob_client.url}?{sas_token}\"\n",
    "\n",
    "\n",
    "def analyze_pdf_to_markdown(sas_url: str) -> str:\n",
    "    \"\"\"Document Intelligenceë¥¼ ì‚¬ìš©í•´ Markdownìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    poller = di_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-layout\",\n",
    "        body=AnalyzeDocumentRequest(url_source=sas_url),\n",
    "        output_content_format='markdown'\n",
    "    )\n",
    "    result = poller.result()\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def request_gpt(prompt: str) -> str:\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': gpt_api_key\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    'ë„ˆëŠ” HTML í…Œì´ë¸”ì„ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•´.  '\n",
    "                    'í•­ëª©ê³¼ ê°’ì„ \"êµ¬ë¶„: ë‚´ìš©\" ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ë§ê³ , ì›ë˜ í…Œì´ë¸”ì—ì„œ ì“°ì¸ í•­ëª©ëª…ì„ ê·¸ëŒ€ë¡œ keyë¡œ ì‚¬ìš©í•´.  '\n",
    "                    'ì˜ˆ: \"ì„ëŒ€ì¡°ê±´ : ë‚´ìš©\", \"ì„ëŒ€ë³´ì¦ê¸ˆ-ì›”ì„ëŒ€ë£Œ ì „í™˜ : ë‚´ìš©\"ì²˜ëŸ¼.  '\n",
    "                    'í•­ëª©ì´ ë°˜ë³µë˜ëŠ” ê²½ìš°ì—ëŠ” êµ¬ë¶„ìë¥¼ ë¶™ì—¬ì„œ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì¤˜.  '\n",
    "                    'ë¶ˆí•„ìš”í•œ ìš”ì•½ì´ë‚˜ ë„ì…ë¶€ ì—†ì´ í‘œì˜ í•µì‹¬ ë‚´ìš©ë§Œ ë³€í™˜í•´ì¤˜.'\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    response = requests.post(gpt_endpoint, headers=headers, json=body)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(\"âŒ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code, response.text)\n",
    "        return \"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "def convert_md_tables_with_llm_parallel(md_text: str, max_workers=5) -> str:\n",
    "    soup = BeautifulSoup(md_text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_strs = [str(table) for table in tables]\n",
    "    unique_tables = list(set(table_strs))\n",
    "    table_to_text = {}\n",
    "\n",
    "    def process_table(table_html):\n",
    "        prompt = (\n",
    "            f\"ë‹¤ìŒ HTML í…Œì´ë¸”ì˜ ë‚´ìš©ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë³€í™˜í•´ì¤˜.\\n\\n{table_html}\"\n",
    "        )\n",
    "        result = request_gpt(prompt)\n",
    "        return table_html, result\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_table, tbl) for tbl in unique_tables]\n",
    "        for future in as_completed(futures):\n",
    "            tbl_html, gpt_result = future.result()\n",
    "            table_to_text[tbl_html] = gpt_result\n",
    "\n",
    "    for original_table in table_strs:\n",
    "        if original_table in table_to_text:\n",
    "            md_text = md_text.replace(original_table, table_to_text[original_table])\n",
    "\n",
    "    return md_text\n",
    "\n",
    "\n",
    "def preprocess_markdown_headers(md_text: str) -> str:\n",
    "    md_text = re.sub(r'^(#{1,6}\\s*â– ?\\s*[^:\\n]+):\\s*(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    md_text = re.sub(r'^(â– \\s*\\([^)]+\\))\\s+(.+)$', r'\\1\\n\\2', md_text, flags=re.MULTILINE)\n",
    "    return md_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì²˜ë¦¬í•  PDF íŒŒì¼ ìˆ˜: 2\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ë¶™ì„1. (25.03.28.)ì²­ë…„ë§¤ì…ì„ëŒ€ ì…ì£¼ì ëª¨ì§‘ê³µê³ ë¬¸_ê²Œì‹œìš©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blank\\AppData\\Local\\Temp\\ipykernel_4136\\1607090331.py:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  expiry=datetime.utcnow() + timedelta(minutes=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\n",
      "âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\n",
      "âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: C:\\Users\\blank\\OneDrive\\ë°”íƒ• í™”ë©´\\2ì°¨ í”„ë¡œì íŠ¸ ë ˆí¬\\í™ì›ë‹˜ ëª¨ë¸\\markdowns\\ë¶™ì„1. (25.03.28.)ì²­ë…„ë§¤ì…ì„ëŒ€ ì…ì£¼ì ëª¨ì§‘ê³µê³ ë¬¸_ê²Œì‹œìš©.md\n",
      "\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: ì•„ì¸ ìŠ¤í…Œì´ì˜ë“±í¬_ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸\n",
      "âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\n",
      "âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\n",
      "âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: C:\\Users\\blank\\OneDrive\\ë°”íƒ• í™”ë©´\\2ì°¨ í”„ë¡œì íŠ¸ ë ˆí¬\\í™ì›ë‹˜ ëª¨ë¸\\markdowns\\ì•„ì¸ ìŠ¤í…Œì´ì˜ë“±í¬_ì…ì£¼ìëª¨ì§‘ê³µê³ ë¬¸.md\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì „ì²´ ì²˜ë¦¬ ë£¨í”„\n",
    "pdf_files = glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
    "print(f\"ğŸ” ì²˜ë¦¬í•  PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    blob_name = f\"{filename}.pdf\"\n",
    "    md_path = os.path.join(MD_FOLDER, f\"{filename}.md\")\n",
    "\n",
    "    print(f\"\\nğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}\")\n",
    "\n",
    "    # 1. ì—…ë¡œë“œ ë° SAS URL ìƒì„±\n",
    "    sas_url = upload_pdf_to_blob(pdf_path, blob_name)\n",
    "    print(\"âœ… Blob ì—…ë¡œë“œ ë° SAS URL ì™„ë£Œ\")\n",
    "\n",
    "    # 2. Markdown ë³€í™˜\n",
    "    md_content = analyze_pdf_to_markdown(sas_url)\n",
    "    print(\"âœ… Document Intelligence ë¶„ì„ ì™„ë£Œ\")\n",
    "\n",
    "    # 3. GPT í…Œì´ë¸” ë³€í™˜\n",
    "    md_with_tables = convert_md_tables_with_llm_parallel(md_content)\n",
    "    print(\"âœ… GPT í…Œì´ë¸” ë³€í™˜ ì™„ë£Œ\")\n",
    "\n",
    "    # 4. í—¤ë” ì „ì²˜ë¦¬\n",
    "    final_md = preprocess_markdown_headers(md_with_tables)\n",
    "\n",
    "    # 5. ì €ì¥\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_md)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {md_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining with the Vocab Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë“  íŒŒì¼ì´ 'mixed_md' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def merge_with_glossary(glossary_path, input_folder, output_folder):\n",
    "    # ìš©ì–´ì§‘ ì½ê¸°\n",
    "    with open(glossary_path, 'r', encoding='utf-8') as glossary_file:\n",
    "        glossary_content = glossary_file.read().strip()\n",
    "\n",
    "    # ì¶œë ¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # í´ë” ë‚´ì˜ ëª¨ë“  md íŒŒì¼ ì²˜ë¦¬\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.md') and filename != os.path.basename(glossary_path):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                file_content = f.read().strip()\n",
    "\n",
    "            # êµ¬ë¶„ì„ ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ í•©ì¹˜ê¸°\n",
    "            separator = \"\\n\\n---\\n\\n\"\n",
    "            merged_content = file_content + separator + glossary_content\n",
    "\n",
    "            # ì¶œë ¥ ê²½ë¡œ\n",
    "            output_path = os.path.join(output_folder, f\"{filename}\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(merged_content)\n",
    "\n",
    "    print(f\"ëª¨ë“  íŒŒì¼ì´ '{output_folder}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "merge_with_glossary(\n",
    "    glossary_path=\"ìš©ì–´ì§‘.md\",\n",
    "    input_folder=\"processed_gpt\",   # ì—¬ê¸°ì— md íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œ ì…ë ¥\n",
    "    output_folder=\"mixed_md\"      # ê²°ê³¼ ì €ì¥ í´ë”\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# mixed_md ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "mixed_md_dir = 'mixed_md'\n",
    "\n",
    "# fin_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  md íŒŒì¼ ì°¾ê¸°\n",
    "fin_files = glob.glob(os.path.join(mixed_md_dir, 'fin_*.md'))\n",
    "\n",
    "# ê° íŒŒì¼ì— ëŒ€í•´ ì²˜ë¦¬\n",
    "for old_path in fin_files:\n",
    "    # íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ\n",
    "    file_name = os.path.basename(old_path)\n",
    "    \n",
    "    # fin_ ì ‘ë‘ì‚¬ ì œê±°\n",
    "    new_file_name = file_name.replace('fin_', '', 1)\n",
    "    \n",
    "    # ìƒˆ íŒŒì¼ ê²½ë¡œ ìƒì„±\n",
    "    new_path = os.path.join(mixed_md_dir, new_file_name)\n",
    "    \n",
    "    try:\n",
    "        # íŒŒì¼ ì´ë¦„ ë³€ê²½\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"{file_name} â†’ {new_file_name} ì´ë¦„ ë³€ê²½ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} ì´ë¦„ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "print(\"ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header Spliiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì´ Markdown íŒŒì¼ ìˆ˜: 17\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (GH)[ì£¼íƒ] 25ë…„ 1ì°¨ ê¸°ì¡´ì£¼íƒ ë“± ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ (ê²½ê¸°ë„ 27ê°œ ì‹œêµ°)\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (GH)[ì£¼íƒ] 25ë…„ 1ì°¨ ì²­ë…„ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)(ì¸ì²œ) ì²­ë…„.ì‹ í˜¼ë¶€ë¶€ ë§¤ì…ì„ëŒ€ë¦¬ì¸ ì£¼íƒ ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)2025ë…„ ì²­ë…„ ì „ì„¸ì„ëŒ€ 1ìˆœìœ„ ì…ì£¼ì ìˆ˜ì‹œëª¨ì§‘\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ê²½ê¸°ë¶ë¶€] 25ë…„ 1ì°¨ ì²­ë…„ë§¤ì…ì„ëŒ€ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì„œìš¸ì§€ì—­ë³¸ë¶€] 25ë…„ 1ì°¨ ì²­ë…„ ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì •ì •ê³µê³ ][ê²½ê¸°ë‚¨ë¶€] 25ë…„ 1ì°¨ ì²­ë…„ ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì •ì •ê³µê³ ][ì„œìš¸ì§€ì—­ë³¸ë¶€] ê³ ë ¹ì ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì •ì •ê³µê³ ][ì„œìš¸ì§€ì—­ë³¸ë¶€] ê¸°ì¡´ì£¼íƒ ë“± ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì •ì •ê³µê³ ]êµ¬ë¦¬ê°ˆë§¤ ì´ìŠ¤íŠ¸í 10ë…„ ê³µê³µì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)[ì •ì •ê³µê³ ]ì „ë¶ ë‚¨ë¶€ê¶Œ ì§€ì—­ êµ­ë¯¼ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)ê´€ì•…ë´‰ì²œ H-1ã†2ã†3BL í–‰ë³µì£¼íƒ ì…ì£¼ìê²©ì™„í™” ì¶”ê°€ëª¨ì§‘\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)êµ°í¬ì‹œ ì˜êµ¬ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)ê¹€í¬í•œê°• Ac-05ë¸”ë¡ 10ë…„ ê³µê³µì„ëŒ€ì£¼íƒ(ë¦¬ì¸ ) ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ ê³µê³ \n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)ì²­ë…„ì‹ í˜¼ë¶€ë¶€ë§¤ì…ì„ëŒ€ë¦¬ì¸ -2502-ê²½ê¸°ë‚¨ë¶€\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)ì²­ë…„ì‹ í˜¼ë¶€ë¶€ë§¤ì…ì„ëŒ€ë¦¬ì¸ -2502-ê²½ê¸°ë¶ë¶€\n",
      "\n",
      "ğŸš€ ì²˜ë¦¬ ì¤‘: (LH)í™”ì„±ë™íƒ„2 10ë…„ ë¶„ì–‘ì „í™˜ê³µê³µì„ëŒ€ì£¼íƒ(ë¦¬ì¸ ) ì˜ˆë¹„ì…ì£¼ìëª¨ì§‘ê³µê³ ('25.03)\n",
      "\n",
      "âœ… ì „ì²´ ì²­í¬ ìˆ˜: 4933\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ğŸ“ ë§ˆí¬ë‹¤ìš´ í´ë” ê²½ë¡œ (raw stringìœ¼ë¡œ ê²½ë¡œ ì‘ì„±)\n",
    "MARKDOWN_FOLDER = # MD_FOLDER\n",
    "\n",
    "# âœ… ë¶„í•  ë„êµ¬ ì •ì˜\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"section\"),\n",
    "    (\"##\", \"subsection\"),\n",
    "    (\"###\", \"subsubsection\"),\n",
    "    (\"â– \", \"bullet\"),\n",
    "    (\"â€»\", \"bullet\"),\n",
    "    (\"â–ª\", \"subbullet\"),\n",
    "    (\"âœ”\", \"check\")\n",
    "])\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# âœ… ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "all_docs = []\n",
    "\n",
    "# âœ… ëª¨ë“  .md íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "md_files = glob(os.path.join(MARKDOWN_FOLDER, \"*.md\"))\n",
    "print(f\"ğŸ“„ ì´ Markdown íŒŒì¼ ìˆ˜: {len(md_files)}\")\n",
    "\n",
    "# âœ… ê° íŒŒì¼ ì²˜ë¦¬\n",
    "for md_path in md_files:\n",
    "    filename = os.path.splitext(os.path.basename(md_path))[0]  # í™•ì¥ì ì—†ëŠ” íŒŒì¼ëª…\n",
    "\n",
    "    print(f\"\\nğŸš€ ì²˜ë¦¬ ì¤‘: {filename}\")\n",
    "\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_text = f.read()\n",
    "\n",
    "    # 1. Markdown í—¤ë” ê¸°ì¤€ ë¶„í• \n",
    "    header_docs = header_splitter.split_text(md_text)\n",
    "\n",
    "    # 2. ê° ë¬¸ì„œì— íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "    for doc in header_docs:\n",
    "        doc.metadata[\"source\"] = filename\n",
    "\n",
    "    # 3. RecursiveCharacterTextSplitterë¡œ chunk ë¶„í• \n",
    "    for doc in header_docs:\n",
    "        sub_docs = recursive_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for chunk in sub_docs:\n",
    "            all_docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata=doc.metadata  # section, subsection, bullet, source í¬í•¨\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ ì²­í¬ ìˆ˜: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bullet': 'í¬ë§ ì‹œ ê°€êµ¬ì› ìˆ˜ë³´ë‹¤ ì‘ì€ ê·œëª¨ì˜ ì£¼íƒ ì‹ ì²­ ê°€ëŠ¥', 'source': '(GH)[ì£¼íƒ]\\xa025ë…„ 1ì°¨ ê¸°ì¡´ì£¼íƒ ë“± ë§¤ì…ì„ëŒ€ì£¼íƒ ì˜ˆë¹„ì…ì£¼ì ëª¨ì§‘ê³µê³ (ê²½ê¸°ë„ 27ê°œ ì‹œêµ°)'}\n",
      "=========================\n",
      "(ì˜ˆì‹œ: 5ì¸ ê°€êµ¬ì˜ ê²½ìš° 3í˜• ëŒ€ìƒì´ë‚˜, ì‹ ì²­ì í¬ë§ ì‹œ 1í˜•ì´ë‚˜ 2í˜•ìœ¼ë¡œ ì‹ ì²­ ê°€ëŠ¥)\n"
     ]
    }
   ],
   "source": [
    "print(all_docs[11].metadata)\n",
    "print('=========================')\n",
    "print(all_docs[11].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading into Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "embedding_api_version = #embedding_api_version\n",
    "embedding_deployment = #embedding_deployment\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Existing indexes: ['add_new_index', 'new_index', 'new_pdf_all_index', 'pdf_1_index', 'pdf_all_index', 'pdf_with_vocab_new_index']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "load_dotenv()\n",
    "\n",
    "# ğŸ”‘ Azure Cognitive Search í‚¤ í™•ì¸ìš© ì½”ë“œë“œ\n",
    "ai_search_api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "ai_search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=AzureKeyCredential(ai_search_api_key))\n",
    "indexes = [i.name for i in index_client.list_indexes()]\n",
    "print(\"ğŸ“¦ Existing indexes:\", indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Azure AI Search ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ ì—…ë¡œë“œ ì¤‘:   2%|â–         | 103/4933 [00:47<32:39,  2.47it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchField, SearchFieldDataType,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "# âœ… 1. API í‚¤ ë° ì—”ë“œí¬ì¸íŠ¸\n",
    "ai_search_index_name = # \"your-index-name\" \n",
    "\n",
    "ai_search_endpoint = ai_search_endpoint\n",
    "ai_search_api_key = ai_search_api_key\n",
    "\n",
    "\n",
    "# âœ… 2. ì„ë² ë”© ì„¤ì •\n",
    "embedding_deployment = # embedding_deployment\n",
    "embedding_api_version = # embedding_api_version\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_BASE\", None)\n",
    "os.environ.pop(\"BASE_URL\", None)\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=embedding_api_key,\n",
    "    azure_endpoint=embedding_endpoint,\n",
    "    model=embedding_deployment,\n",
    "    openai_api_version=embedding_api_version\n",
    ")\n",
    "\n",
    "# âœ… 3. ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "embedding_dim = 1536\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(name=\"source\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"section\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subsubsection\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"bullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"subbullet\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(name=\"check\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "    SearchField(\n",
    "        name=\"embedding\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=embedding_dim,\n",
    "        vector_search_profile_name=\"default\"\n",
    "    )\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"default\", algorithm_configuration_name=\"my-algorithm\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithm\", kind=VectorSearchAlgorithmKind.HNSW)]\n",
    ")\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=ai_search_index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search\n",
    ")\n",
    "\n",
    "# âœ… 4. ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ìƒì„±\n",
    "index_client = SearchIndexClient(endpoint=ai_search_endpoint, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "if ai_search_index_name in [i.name for i in index_client.list_indexes()]:\n",
    "    index_client.delete_index(ai_search_index_name)\n",
    "    print(\"ğŸ—‘ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ\")\n",
    "\n",
    "index_client.create_index(index)\n",
    "print(\"âœ… Azure AI Search ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# âœ… 5. ë²¡í„° ë°ì´í„° ì—…ë¡œë“œ\n",
    "# ğŸ‘‰ ì—¬ê¸°ì„œ all_docsëŠ” ì´ë¯¸ ë§Œë“¤ì–´ì§„ Document ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •\n",
    "\n",
    "# ì˜ˆ: all_docs = [Document(page_content=..., metadata={...}), ...]\n",
    "\n",
    "search_client = SearchClient(endpoint=ai_search_endpoint, index_name=ai_search_index_name, credential=AzureKeyCredential(ai_search_api_key))\n",
    "\n",
    "batch = []\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for doc in tqdm(all_docs, desc=\"ğŸ“¤ ì—…ë¡œë“œ ì¤‘\"):\n",
    "    vector = embedding.embed_query(doc.page_content)\n",
    "\n",
    "    record = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": doc.page_content,\n",
    "        \"embedding\": vector,\n",
    "        \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "        \"section\": doc.metadata.get(\"section\", \"\"),\n",
    "        \"subsection\": doc.metadata.get(\"subsection\", \"\"),\n",
    "        \"subsubsection\": doc.metadata.get(\"subsubsection\", \"\"),\n",
    "        \"bullet\": doc.metadata.get(\"bullet\", \"\"),\n",
    "        \"subbullet\": doc.metadata.get(\"subbullet\", \"\"),\n",
    "        \"check\": doc.metadata.get(\"check\", \"\")\n",
    "    }\n",
    "\n",
    "    batch.append(record)\n",
    "\n",
    "    if len(batch) >= BATCH_SIZE:\n",
    "        search_client.upload_documents(documents=batch)\n",
    "        batch = []\n",
    "\n",
    "# ë‚¨ì€ ë°ì´í„° ì—…ë¡œë“œ\n",
    "if batch:\n",
    "    search_client.upload_documents(documents=batch)\n",
    "\n",
    "print(\"âœ… ì „ì²´ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
